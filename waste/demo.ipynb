{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b93daa0c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(OllamaLLM(model='deepseek-r1:8b'), 'deepseek-r1:8b'),\n",
       " (OllamaLLM(model='starling-lm'), 'starling-1m'),\n",
       " (OllamaLLM(model='ministral-3'), 'minstral-3'),\n",
       " (OllamaLLM(model='mistral:7b'), 'mistral:7b'),\n",
       " (OllamaLLM(model='phi3:mini'), 'phi3:mini'),\n",
       " (OllamaLLM(model='gemma2:9b'), 'gemma2:9b'),\n",
       " (OllamaLLM(model='gpt-oss:20b'), 'gpt-oss:20b')]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from ai_council.council import *\n",
    "from ai_council.prompts import *\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import PydanticOutputParser\n",
    "\n",
    "\n",
    "[(k[\"llm\"] , k[\"name\"]) for k in MODELS]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "00dc2e8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response from deepseek-r1:8b\n",
      "Response generated by deepseek-r1:8b with id r_0\n",
      "Response from starling-1m\n",
      "Response generated by starling-1m with id r_1\n",
      "Response from minstral-3\n",
      "Response generated by minstral-3 with id r_2\n",
      "Response from mistral:7b\n",
      "Response generated by mistral:7b with id r_3\n",
      "Response from phi3:mini\n",
      "Response generated by phi3:mini with id r_4\n",
      "Response from gpt-oss:20b\n",
      "Response generated by gpt-oss:20b with id r_6\n",
      "{'response_id': 'r_0', 'model_id': 'expert_1', 'text': '**Final answer:** Not in context.\\n\\n**Reasoning:** The provided context does not contain any information about female mountaineers or the first woman to climb Mount Everest.\\n\\n**Snippets used:** none'}\n",
      "{'response_id': 'r_1', 'model_id': 'expert_2', 'text': '1. Junko Tabei was the first female mountaineer to climb Mount Everest.\\n2. Reasoning: The context does not provide specific information about the first female mountaineer to climb Mount Everest. However, Junko Tabei is a well-known figure in mountaineering history and she accomplished this feat on May 16, 1975, making her the first woman to climb the highest peak in the world.\\n3. Snippets used: None (the information about Junko Tabei was not included in the context).'}\n",
      "{'response_id': 'r_2', 'model_id': 'expert_3', 'text': 'Not in context.'}\n",
      "{'response_id': 'r_3', 'model_id': 'expert_4', 'text': ' 1. Junko Tabei was the first female mountaineer to climb Mount Everest. [2]\\n2. This information is found in the second paragraph of the context.\\n3. Snippets used: 2'}\n",
      "{'response_id': 'r_4', 'model_id': 'expert_5', 'text': '1. Not in context. (The provided text does not contain information about a female mountaineer who first climbed Mount Everest.)\\n2. The given CONTEXT lacks any mention of historical figures or events related to mountaineering achievements, especially concerning gender mileststaticion such as being the first woman to summit Mount Everest. \\n3. #none#'}\n",
      "{'response_id': 'r_6', 'model_id': 'expert_6', 'text': '1. Not in context.  \\n2. No relevant information was provided.  \\n3. none'}\n"
     ]
    }
   ],
   "source": [
    "input_prompt = \"Who was the first female mountaineer to climb Mount Everest\"\n",
    "responses , user_prompt = generate_expert_response(input_prompt, \"\")\n",
    "for response in responses:\n",
    "    print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e368433e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "deepseek-r1:8b\n",
      "Scoring complete for r_0 by deepseek-r1:8b\n",
      "Scoring complete for r_1 by deepseek-r1:8b\n",
      "Scoring complete for r_2 by deepseek-r1:8b\n",
      "Scoring complete for r_3 by deepseek-r1:8b\n",
      "Scoring complete for r_4 by deepseek-r1:8b\n",
      "Scoring complete for r_6 by deepseek-r1:8b\n",
      "starling-1m\n",
      "Scoring complete for r_0 by starling-1m\n",
      "Scoring complete for r_1 by starling-1m\n",
      "Scoring complete for r_2 by starling-1m\n",
      "Scoring complete for r_3 by starling-1m\n",
      "Scoring complete for r_4 by starling-1m\n",
      "Scoring complete for r_6 by starling-1m\n",
      "minstral-3\n",
      "Scoring complete for r_0 by minstral-3\n",
      "Scoring complete for r_1 by minstral-3\n",
      "Scoring complete for r_2 by minstral-3\n",
      "Scoring complete for r_3 by minstral-3\n",
      "Scoring complete for r_4 by minstral-3\n",
      "Scoring complete for r_6 by minstral-3\n",
      "mistral:7b\n",
      "Scoring complete for r_0 by mistral:7b\n",
      "Scoring complete for r_1 by mistral:7b\n",
      "Scoring complete for r_2 by mistral:7b\n",
      "Scoring complete for r_3 by mistral:7b\n",
      "Scoring complete for r_4 by mistral:7b\n",
      "Scoring complete for r_6 by mistral:7b\n",
      "phi3:mini\n",
      "Scoring complete for r_0 by phi3:mini\n",
      "Scoring complete for r_1 by phi3:mini\n",
      "Scoring complete for r_2 by phi3:mini\n",
      "Scoring complete for r_3 by phi3:mini\n",
      "Scoring complete for r_4 by phi3:mini\n",
      "Scoring complete for r_6 by phi3:mini\n",
      "gpt-oss:20b\n",
      "Scoring complete for r_0 by gpt-oss:20b\n",
      "Scoring complete for r_1 by gpt-oss:20b\n",
      "Scoring complete for r_2 by gpt-oss:20b\n",
      "Scoring complete for r_3 by gpt-oss:20b\n",
      "Scoring complete for r_4 by gpt-oss:20b\n",
      "Scoring complete for r_6 by gpt-oss:20b\n"
     ]
    }
   ],
   "source": [
    "scoring_matrix = generate_scores(responses, user_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "20649725",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_variables=['output_format', 'responses', 'scoring_matrix', 'user_prompt'] input_types={} partial_variables={} messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['output_format', 'responses', 'scoring_matrix', 'user_prompt'], input_types={}, partial_variables={}, template='\\n    SYSTEM: You are an independent auditor whose job is to inspect a scoring matrix produced by peer models and detect bias, collusion, or anomalous scoring patterns. Return only the JSON described below.\\n\\n    USER: We provide:\\n    1) original_prompt: {user_prompt}\\n    2) responses: a JSON list of response objects:\\n    {responses}\\n    3) scoring_matrix: a JSON object where keys are scorer_ids and values are dictionaries mapping response_id -> score_obj\\n    e.g., {scoring_matrix}\\n\\n    Task:\\n    1) Inspect scoring patterns for the following anomalies:\\n    - Self-scoring or allowed self-favoring (scorer giving systematically higher scores to a single partner)\\n    - Collusion: two or more scorers consistently upvoting each other across many prompts (pattern detection)\\n    - Extreme scorers: scorer that always gives very high (>=4.5) or very low (<=1.5) totals while variance is near zero\\n    - Outliers: scorer scores that deviate > 2 std from the mean for a response\\n\\n    2) For each detected anomaly produce a corrective action:\\n    - normalization factor for that scorer (multiply all their scores by that factor)\\n    - or flag for human review (if severe)\\n    - or drop scorer from aggregation for this prompt\\n\\n    Return JSON **only**:\\n    {output_format}\\n'), additional_kwargs={})]\n"
     ]
    }
   ],
   "source": [
    "audit , audit_prompt= generate_audit_report(input_prompt, responses, scoring_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b928630f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "```json\n",
      "{\n",
      "  \"audit_id\": \"e4673089-b7a2-415f-8d8c-95356d28165f\",\n",
      "  \"flags\": [\n",
      "    {\n",
      "      \"scorer_id\": \"phi3:mini\",\n",
      "      \"issue\": \"Extreme Scorer\",\n",
      "      \"severity\": \"High\"\n",
      "    }\n",
      "  ],\n",
      "  \"drops\": [],\n",
      "  \"explanation\": \"The scorer 'phi3:mini' consistently awards extremely low scores (generally below 3.5) across multiple prompts, indicating a possible bias or misunderstanding of the scoring criteria. This warrants further investigation and potential normalization.\",\n",
      "  \"normalization\": {\n",
      "    \"phi3:mini\": 1.5 \n",
      "  }\n",
      "}\n",
      "```\n"
     ]
    }
   ],
   "source": [
    "print(audit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3798b9db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  \"audit_id\": \"e4673089-b7a2-415f-8d8c-95356d28165f\",\n",
      "  \"flags\": [\n",
      "    {\n",
      "      \"scorer_id\": \"phi3:mini\",\n",
      "      \"issue\": \"Extreme Scorer\",\n",
      "      \"severity\": \"High\"\n",
      "    }\n",
      "  ],\n",
      "  \"drops\": [],\n",
      "  \"explanation\": \"The scorer 'phi3:mini' consistently awards extremely low scores (generally below 3.5) across multiple prompts, indicating a possible bias or misunderstanding of the scoring criteria. This warrants further investigation and potential normalization.\",\n",
      "  \"normalization\": {\n",
      "    \"phi3:mini\": 1.5 \n",
      "  }\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print_with_bold(extract_first_curly_balanced(audit))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "de62a2d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'audit_id': 'e4673089-b7a2-415f-8d8c-95356d28165f', 'flags': [{'scorer_id': 'phi3:mini', 'issue': 'Extreme Scorer', 'severity': 'High'}], 'drops': [], 'explanation': \"The scorer 'phi3:mini' consistently awards extremely low scores (generally below 3.5) across multiple prompts, indicating a possible bias or misunderstanding of the scoring criteria. This warrants further investigation and potential normalization.\", 'normalization': {'phi3:mini': 1.5}}\n"
     ]
    }
   ],
   "source": [
    "audit_report = ast.literal_eval('{' + extract_first_curly_balanced(audit) + '}')\n",
    "print(audit_report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ef88d142",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'phi3:mini': 1.5}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "audit_report['normalization']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "65be9857",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"deepseek-r1:8b\": {\n",
      "        \"r_0\": {\n",
      "            \"confidence_estimate\": 0.95,\n",
      "            \"justification\": \"The response is accurate, complete, grounded, reasoned, and clear, correctly handling the empty context without errors.\",\n",
      "            \"scores\": {\n",
      "                \"accuracy\": 5,\n",
      "                \"clarity\": 5,\n",
      "                \"completeness\": 5,\n",
      "                \"grounding\": 5,\n",
      "                \"reasoning\": 5\n",
      "            },\n",
      "            \"total\": 5.0\n",
      "        },\n",
      "        \"r_1\": {\n",
      "            \"confidence_estimate\": 0.9,\n",
      "            \"justification\": \"The response is factually accurate but lacks proper grounding in the context, with flawed reasoning for providing external information.\",\n",
      "            \"scores\": {\n",
      "                \"accuracy\": 5,\n",
      "                \"clarity\": 5,\n",
      "                \"completeness\": 5,\n",
      "                \"grounding\": 1,\n",
      "                \"reasoning\": 1\n",
      "            },\n",
      "            \"total\": 3.6\n",
      "        },\n",
      "        \"r_2\": {\n",
      "            \"confidence_estimate\": 0.9,\n",
      "            \"justification\": \"The response is accurate given the empty context but lacks completeness, grounding, and reasoning, resulting in a low overall score.\",\n",
      "            \"scores\": {\n",
      "                \"accuracy\": 5,\n",
      "                \"clarity\": 4,\n",
      "                \"completeness\": 1,\n",
      "                \"grounding\": 1,\n",
      "                \"reasoning\": 1\n",
      "            },\n",
      "            \"total\": 2.55\n",
      "        },\n",
      "        \"r_3\": {\n",
      "            \"confidence_estimate\": 0.9,\n",
      "            \"justification\": \"The response is accurate, complete, and clear, with adequate grounding and reasoning, though verification of the snippet reference is not possible without context.\",\n",
      "            \"scores\": {\n",
      "                \"accuracy\": 5,\n",
      "                \"clarity\": 5,\n",
      "                \"completeness\": 5,\n",
      "                \"grounding\": 4,\n",
      "                \"reasoning\": 4\n",
      "            },\n",
      "            \"total\": 4.65\n",
      "        },\n",
      "        \"r_4\": {\n",
      "            \"confidence_estimate\": 0.95,\n",
      "            \"justification\": \"The response accurately follows the prompt and context rules, addressing all parts with proper grounding and clear structure.\",\n",
      "            \"scores\": {\n",
      "                \"accuracy\": 5,\n",
      "                \"clarity\": 5,\n",
      "                \"completeness\": 5,\n",
      "                \"grounding\": 5,\n",
      "                \"reasoning\": 5\n",
      "            },\n",
      "            \"total\": 5.0\n",
      "        },\n",
      "        \"r_6\": {\n",
      "            \"confidence_estimate\": 0.95,\n",
      "            \"justification\": \"The candidate response accurately and fully addresses the prompt by correctly stating the answer is not in context, ensuring all elements are covered and reasoning is sound based on the absence of information.\",\n",
      "            \"scores\": {\n",
      "                \"accuracy\": 5,\n",
      "                \"clarity\": 5,\n",
      "                \"completeness\": 5,\n",
      "                \"grounding\": 5,\n",
      "                \"reasoning\": 5\n",
      "            },\n",
      "            \"total\": 5.0\n",
      "        }\n",
      "    },\n",
      "    \"gpt-oss:20b\": {\n",
      "        \"r_0\": {\n",
      "            \"confidence_estimate\": 0.99,\n",
      "            \"justification\": \"The response correctly follows the instruction to state \\\"Not in context\\\" when no relevant information is present, fully addressing the prompt with accurate, grounded, clear reasoning.\",\n",
      "            \"scores\": {\n",
      "                \"accuracy\": 5,\n",
      "                \"clarity\": 5,\n",
      "                \"completeness\": 5,\n",
      "                \"grounding\": 5,\n",
      "                \"reasoning\": 5\n",
      "            },\n",
      "            \"total\": 5.0\n",
      "        },\n",
      "        \"r_1\": {\n",
      "            \"confidence_estimate\": 0.85,\n",
      "            \"justification\": \"The answer is factually correct but it does not adhere to the instruction to rely only on context, lacks proper grounding, and the reasoning provided is logically inconsistent.\",\n",
      "            \"scores\": {\n",
      "                \"accuracy\": 5,\n",
      "                \"clarity\": 5,\n",
      "                \"completeness\": 5,\n",
      "                \"grounding\": 1,\n",
      "                \"reasoning\": 2\n",
      "            },\n",
      "            \"total\": 3.75\n",
      "        },\n",
      "        \"r_2\": {\n",
      "            \"confidence_estimate\": 0.95,\n",
      "            \"justification\": \"The response correctly follows the system instruction by stating 'Not in context' when the information is absent, thus it is accurate, complete, well-grounded, logically sound, and clear.\",\n",
      "            \"scores\": {\n",
      "                \"accuracy\": 5,\n",
      "                \"clarity\": 5,\n",
      "                \"completeness\": 5,\n",
      "                \"grounding\": 5,\n",
      "                \"reasoning\": 4\n",
      "            },\n",
      "            \"total\": 4.85\n",
      "        },\n",
      "        \"r_3\": {\n",
      "            \"confidence_estimate\": 0.8,\n",
      "            \"justification\": \"The answer is factually correct and fully addresses the question, but it incorrectly cites a non-existent context snippet, resulting in poor grounding and weak reasoning.\",\n",
      "            \"scores\": {\n",
      "                \"accuracy\": 5,\n",
      "                \"clarity\": 5,\n",
      "                \"completeness\": 5,\n",
      "                \"grounding\": 1,\n",
      "                \"reasoning\": 2\n",
      "            },\n",
      "            \"total\": 3.75\n",
      "        },\n",
      "        \"r_4\": {\n",
      "            \"confidence_estimate\": 0.98,\n",
      "            \"justification\": \"The candidate correctly follows the instruction to answer 'Not in context', provides a logical reasoning that the context contains no relevant information, and adheres to the required format with clear structure.\",\n",
      "            \"scores\": {\n",
      "                \"accuracy\": 5,\n",
      "                \"clarity\": 5,\n",
      "                \"completeness\": 5,\n",
      "                \"grounding\": 5,\n",
      "                \"reasoning\": 5\n",
      "            },\n",
      "            \"total\": 5.0\n",
      "        },\n",
      "        \"r_6\": {\n",
      "            \"confidence_estimate\": 0.95,\n",
      "            \"justification\": \"The response accurately follows the system instructions by stating \\\"Not in context.\\\" and provides all required sections with clear, concise formatting, thereby satisfying all rubric criteria.\",\n",
      "            \"scores\": {\n",
      "                \"accuracy\": 5,\n",
      "                \"clarity\": 5,\n",
      "                \"completeness\": 5,\n",
      "                \"grounding\": 5,\n",
      "                \"reasoning\": 5\n",
      "            },\n",
      "            \"total\": 5.0\n",
      "        }\n",
      "    },\n",
      "    \"minstral-3\": {\n",
      "        \"r_0\": {\n",
      "            \"confidence_estimate\": 1.0,\n",
      "            \"justification\": \"The response perfectly adhered to the retrieval-grounded instructions, correctly identified the absence of relevant context, and provided a concise, well-structured answer with no factual errors or unsupported claims.\",\n",
      "            \"scores\": {\n",
      "                \"accuracy\": 5,\n",
      "                \"clarity\": 5,\n",
      "                \"completeness\": 5,\n",
      "                \"grounding\": 5,\n",
      "                \"reasoning\": 5\n",
      "            },\n",
      "            \"total\": 5.0\n",
      "        },\n",
      "        \"r_1\": {\n",
      "            \"confidence_estimate\": 0.95,\n",
      "            \"justification\": \"The response provided an accurate fact (Junko Tabei was the first female mountaineer to climb Everest) but failed to adhere to the instruction to ground answers solely in the provided context, instead inventing reasoning and citing no snippets.\",\n",
      "            \"scores\": {\n",
      "                \"accuracy\": 3,\n",
      "                \"clarity\": 4,\n",
      "                \"completeness\": 2,\n",
      "                \"grounding\": 1,\n",
      "                \"reasoning\": 2\n",
      "            },\n",
      "            \"total\": 2.25\n",
      "        },\n",
      "        \"r_2\": {\n",
      "            \"confidence_estimate\": 1.0,\n",
      "            \"justification\": \"The response correctly adheres to the instructions by explicitly stating 'Not in context' when no relevant information was provided, demonstrating full accuracy, completeness, grounding, logical coherence, and clarity.\",\n",
      "            \"scores\": {\n",
      "                \"accuracy\": 5,\n",
      "                \"clarity\": 5,\n",
      "                \"completeness\": 5,\n",
      "                \"grounding\": 5,\n",
      "                \"reasoning\": 5\n",
      "            },\n",
      "            \"total\": 5.0\n",
      "        },\n",
      "        \"r_3\": {\n",
      "            \"confidence_estimate\": 1.0,\n",
      "            \"justification\": \"The response is factually incorrect because the context provided is empty, making any claim unsupported; the answer violates the retrieval-grounded instruction by inventing a fact.\",\n",
      "            \"scores\": {\n",
      "                \"accuracy\": 1,\n",
      "                \"clarity\": 2,\n",
      "                \"completeness\": 1,\n",
      "                \"grounding\": 1,\n",
      "                \"reasoning\": 1\n",
      "            },\n",
      "            \"total\": 1.05\n",
      "        },\n",
      "        \"r_4\": {\n",
      "            \"confidence_estimate\": 1.0,\n",
      "            \"justification\": \"The response perfectly adheres to the instructions by explicitly stating 'Not in context' and providing a clear, concise justification tied to the absence of relevant information in the provided context, with no deviations or unsupported claims.\",\n",
      "            \"scores\": {\n",
      "                \"accuracy\": 5,\n",
      "                \"clarity\": 5,\n",
      "                \"completeness\": 5,\n",
      "                \"grounding\": 5,\n",
      "                \"reasoning\": 5\n",
      "            },\n",
      "            \"total\": 5.0\n",
      "        },\n",
      "        \"r_6\": {\n",
      "            \"confidence_estimate\": 1.0,\n",
      "            \"justification\": \"The response perfectly adhered to the instructions by explicitly stating 'Not in context' and providing a clear, concise justification with no unsupported claims or deviations.\",\n",
      "            \"scores\": {\n",
      "                \"accuracy\": 5,\n",
      "                \"clarity\": 5,\n",
      "                \"completeness\": 5,\n",
      "                \"grounding\": 5,\n",
      "                \"reasoning\": 5\n",
      "            },\n",
      "            \"total\": 5.0\n",
      "        }\n",
      "    },\n",
      "    \"mistral:7b\": {\n",
      "        \"r_0\": {\n",
      "            \"confidence_estimate\": 1,\n",
      "            \"justification\": \"The response accurately states that the provided context does not contain information about the first female mountaineer to climb Mount Everest. It fully addresses all parts of the prompt and provides a clear and well-structured answer, with no logical errors or unsupported claims.\",\n",
      "            \"scores\": {\n",
      "                \"accuracy\": 5,\n",
      "                \"clarity\": 5,\n",
      "                \"completeness\": 5,\n",
      "                \"grounding\": 5,\n",
      "                \"reasoning\": 5\n",
      "            },\n",
      "            \"total\": 5.0\n",
      "        },\n",
      "        \"r_1\": {\n",
      "            \"confidence_estimate\": 0.9,\n",
      "            \"justification\": \"The response accurately states that Junko Tabei was the first female mountaineer to climb Mount Everest, despite not finding the information in the context. However, the reasoning provided could be more straightforward and clearer. The reference to external sources is also missing but the given response does not include unsupported claims.\",\n",
      "            \"scores\": {\n",
      "                \"accuracy\": 5,\n",
      "                \"clarity\": 5,\n",
      "                \"completeness\": 5,\n",
      "                \"grounding\": 4,\n",
      "                \"reasoning\": 3\n",
      "            },\n",
      "            \"total\": 4.5\n",
      "        },\n",
      "        \"r_2\": {\n",
      "            \"confidence_estimate\": 1.0,\n",
      "            \"justification\": \"The response accurately states that the information about who was the first female mountaineer to climb Mount Everest is not in the provided context.\",\n",
      "            \"scores\": {\n",
      "                \"accuracy\": 5,\n",
      "                \"clarity\": 5,\n",
      "                \"completeness\": 5,\n",
      "                \"grounding\": 5,\n",
      "                \"reasoning\": 5\n",
      "            },\n",
      "            \"total\": 5.0\n",
      "        },\n",
      "        \"r_3\": {\n",
      "            \"confidence_estimate\": 1.0,\n",
      "            \"justification\": \"The candidate's response is factually accurate, fully addresses all parts of the prompt, provides clear references to the context, demonstrates sound reasoning and exhibits a high level of clarity.\",\n",
      "            \"scores\": {\n",
      "                \"accuracy\": 5,\n",
      "                \"clarity\": 5,\n",
      "                \"completeness\": 5,\n",
      "                \"grounding\": 5,\n",
      "                \"reasoning\": 5\n",
      "            },\n",
      "            \"total\": 5.0\n",
      "        },\n",
      "        \"r_4\": {\n",
      "            \"confidence_estimate\": 1.0,\n",
      "            \"justification\": \"The response is accurate as it correctly identifies that the provided context does not contain information about the first female mountaineer to climb Mount Everest. The response fully addresses all parts of the prompt, and provides clear reasoning and well-structured content.\",\n",
      "            \"scores\": {\n",
      "                \"accuracy\": 5,\n",
      "                \"clarity\": 5,\n",
      "                \"completeness\": 5,\n",
      "                \"grounding\": 5,\n",
      "                \"reasoning\": 5\n",
      "            },\n",
      "            \"total\": 5.0\n",
      "        },\n",
      "        \"r_6\": {\n",
      "            \"confidence_estimate\": 1.0,\n",
      "            \"justification\": \"The response is factually correct as it correctly identifies that the context does not contain information about the first female mountaineer to climb Mount Everest. It addresses all parts of the prompt and provides a clear and concise answer without any unsupported claims or flawed reasoning.\",\n",
      "            \"scores\": {\n",
      "                \"accuracy\": 5,\n",
      "                \"clarity\": 5,\n",
      "                \"completeness\": 5,\n",
      "                \"grounding\": 5,\n",
      "                \"reasoning\": 5\n",
      "            },\n",
      "            \"total\": 5.0\n",
      "        }\n",
      "    },\n",
      "    \"phi3:mini\": {\n",
      "        \"r_0\": {\n",
      "            \"confidence_estimate\": 0.0,\n",
      "            \"justification\": 0,\n",
      "            \"scores\": {\n",
      "                \"accuracy\": 0,\n",
      "                \"clarity\": 0,\n",
      "                \"completeness\": 0,\n",
      "                \"grounding\": 0,\n",
      "                \"reasoning\": 0\n",
      "            },\n",
      "            \"total\": 0.0\n",
      "        },\n",
      "        \"r_1\": {\n",
      "            \"confidence_estimate\": 0.75,\n",
      "            \"justification\": \"The response accurately identifies Junko Tabei as a notable figure in mountaineering and her achievement of climbing Everest aligns with historical facts; however, the reasoning is flawed due to lack of direct evidence from context provided, grounding lacks citation or source reference.\",\n",
      "            \"scores\": {\n",
      "                \"accuracy\": 5,\n",
      "                \"clarity\": 5,\n",
      "                \"completeness\": 4,\n",
      "                \"grounding\": 1,\n",
      "                \"reasoning\": 2\n",
      "            },\n",
      "            \"total\": 3.5\n",
      "        },\n",
      "        \"r_2\": {\n",
      "            \"confidence_estimate\": 0,\n",
      "            \"justification\": 0,\n",
      "            \"scores\": {\n",
      "                \"accuracy\": 0,\n",
      "                \"clarity\": 0,\n",
      "                \"completeness\": 0,\n",
      "                \"grounding\": 0,\n",
      "                \"reasoning\": 0\n",
      "            },\n",
      "            \"total\": 0.0\n",
      "        },\n",
      "        \"r_3\": {\n",
      "            \"confidence_estimate\": 0.95,\n",
      "            \"justification\": \"The response accurately identifies Junko Tabei as the first female to climb Everest but fails in providing proper grounding for its claim.\",\n",
      "            \"scores\": {\n",
      "                \"accuracy\": 5,\n",
      "                \"clarity\": 5,\n",
      "                \"completeness\": 2,\n",
      "                \"grounding\": 1,\n",
      "                \"reasoning\": 5\n",
      "            },\n",
      "            \"total\": 3.45\n",
      "        },\n",
      "        \"r_4\": {\n",
      "            \"confidence_estimate\": 0.95,\n",
      "            \"justification\": \"The candidate response accurately identifies that the information is not in context and explains why it cannot answer based on provided texts.\",\n",
      "            \"scores\": {\n",
      "                \"accuracy\": 0,\n",
      "                \"clarity\": 0,\n",
      "                \"completeness\": 0,\n",
      "                \"grounding\": 0,\n",
      "                \"reasoning\": 0\n",
      "            },\n",
      "            \"total\": 0.0\n",
      "        },\n",
      "        \"r_6\": {\n",
      "            \"confidence_estimate\": 0.0,\n",
      "            \"justification\": 0,\n",
      "            \"scores\": {\n",
      "                \"accuracy\": 0,\n",
      "                \"clarity\": 0,\n",
      "                \"completeness\": 0,\n",
      "                \"grounding\": 0,\n",
      "                \"reasoning\": 0\n",
      "            },\n",
      "            \"total\": 0.0\n",
      "        }\n",
      "    },\n",
      "    \"starling-1m\": {\n",
      "        \"r_0\": {\n",
      "            \"confidence_estimate\": 0.8,\n",
      "            \"justification\": \"The response correctly identifies that the context provided does not contain information about female mountaineers or Mount Everest climbers. However, it could have mentioned that the answer to the question (Edna Brightwell) is found in external sources and not within the context itself, as the prompt suggests. The response shows clarity and conciseness but misses the opportunity to provide additional relevant information. Grounding was affected by not referencing external sources.\",\n",
      "            \"scores\": {\n",
      "                \"accuracy\": 5,\n",
      "                \"clarity\": 5,\n",
      "                \"completeness\": 3,\n",
      "                \"grounding\": 2,\n",
      "                \"reasoning\": 4\n",
      "            },\n",
      "            \"total\": 3.75\n",
      "        },\n",
      "        \"r_1\": {\n",
      "            \"confidence_estimate\": 0.98,\n",
      "            \"justification\": \"The response accurately identifies Junko Tabei as the first female mountaineer to climb Mount Everest and provides the correct date of her achievement. However, the grounding score is reduced by one point because the response mentions that the information about Junko Tabei was not included in the context, which is incorrect; the context merely prohibits making statements without evidence, rather than stating that there's no relevant information available.\",\n",
      "            \"scores\": {\n",
      "                \"accuracy\": 5,\n",
      "                \"clarity\": 5,\n",
      "                \"completeness\": 5,\n",
      "                \"grounding\": 4,\n",
      "                \"reasoning\": 5\n",
      "            },\n",
      "            \"total\": 4.8\n",
      "        },\n",
      "        \"r_2\": {\n",
      "            \"confidence_estimate\": 0.25,\n",
      "            \"justification\": \"The response does not answer the user's question about the first female mountaineer to climb Mount Everest. It simply states 'Not in context,' which is accurate but does not provide any relevant information or historical fact. Additionally, the response lacks any evidence or reasoning that could be checked against a verifiable source. The response is clear and concise, but it misses the core part of the prompt, resulting in low scores across all categories.\",\n",
      "            \"scores\": {\n",
      "                \"accuracy\": 1,\n",
      "                \"clarity\": 1,\n",
      "                \"completeness\": 1,\n",
      "                \"grounding\": 1,\n",
      "                \"reasoning\": 1\n",
      "            },\n",
      "            \"total\": 1.0\n",
      "        },\n",
      "        \"r_3\": {\n",
      "            \"$defs\": {\n",
      "                \"Scores\": {\n",
      "                    \"properties\": {\n",
      "                        \"accuracy\": {\n",
      "                            \"title\": \"Accuracy\",\n",
      "                            \"type\": \"integer\"\n",
      "                        },\n",
      "                        \"clarity\": {\n",
      "                            \"title\": \"Clarity\",\n",
      "                            \"type\": \"integer\"\n",
      "                        },\n",
      "                        \"completeness\": {\n",
      "                            \"title\": \"Completeness\",\n",
      "                            \"type\": \"integer\"\n",
      "                        },\n",
      "                        \"grounding\": {\n",
      "                            \"title\": \"Grounding\",\n",
      "                            \"type\": \"integer\"\n",
      "                        },\n",
      "                        \"reasoning\": {\n",
      "                            \"title\": \"Reasoning\",\n",
      "                            \"type\": \"integer\"\n",
      "                        }\n",
      "                    },\n",
      "                    \"required\": [\n",
      "                        \"accuracy\",\n",
      "                        \"completeness\",\n",
      "                        \"grounding\",\n",
      "                        \"reasoning\",\n",
      "                        \"clarity\"\n",
      "                    ],\n",
      "                    \"title\": \"Scores\",\n",
      "                    \"type\": \"object\"\n",
      "                },\n",
      "                \"properties\": {\n",
      "                    \"confidence_estimate\": {\n",
      "                        \"title\": \"Confidence Estimate\",\n",
      "                        \"type\": \"number\"\n",
      "                    },\n",
      "                    \"justification\": {\n",
      "                        \"title\": \"Justification\",\n",
      "                        \"type\": \"string\"\n",
      "                    },\n",
      "                    \"required\": [\n",
      "                        \"scores\",\n",
      "                        \"confidence_estimate\",\n",
      "                        \"justification\"\n",
      "                    ],\n",
      "                    \"scores\": {\n",
      "                        \"$ref\": \"#/$defs/Scores\"\n",
      "                    }\n",
      "                }\n",
      "            },\n",
      "            \"confidence_estimate\": 0.98,\n",
      "            \"justification\": \"The candidate response correctly identifies Junko Tabei as the first female mountaineer to climb Mount Everest and cites the context (snippet 2) to support this fact. The response is concise and well-structured, with clear information that addresses all parts of the prompt. It demonstrates accurate information based on known facts, full coverage of the topic, strong grounding in the provided context, sound reasoning, and clear communication.\",\n",
      "            \"scores\": {\n",
      "                \"accuracy\": 5,\n",
      "                \"clarity\": 5,\n",
      "                \"completeness\": 5,\n",
      "                \"grounding\": 5,\n",
      "                \"reasoning\": 5\n",
      "            },\n",
      "            \"total\": 5.0\n",
      "        },\n",
      "        \"r_4\": {\n",
      "            \"confidence_estimate\": 0.75,\n",
      "            \"justification\": \"The response correctly identifies that the given context does not provide information about the first female mountaineer to climb Mount Everest. However, it lacks grounding as it fails to cite any snippets or reference verifiable sources. The reasoning and completeness are somewhat satisfactory as the answer acknowledges the absence of such information in the provided context. Clarity is good as the response is concise and well-structured.\",\n",
      "            \"scores\": {\n",
      "                \"accuracy\": 3,\n",
      "                \"clarity\": 4,\n",
      "                \"completeness\": 2,\n",
      "                \"grounding\": 1,\n",
      "                \"reasoning\": 3\n",
      "            },\n",
      "            \"total\": 2.4\n",
      "        },\n",
      "        \"r_6\": {\n",
      "            \"confidence_estimate\": 0.25,\n",
      "            \"justification\": \"The response does not provide any information about the first female mountaineer to climb Mount Everest and instead states that the context is 'Not in context.' The answer lacks factual accuracy, fails to address any part of the prompt, offers no grounding or citation of relevant information, displays flawed reasoning by stating there's no relevant information available, and does not provide a clear or concise response. Due to these reasons, it scores poorly on all criteria.\",\n",
      "            \"scores\": {\n",
      "                \"accuracy\": 1,\n",
      "                \"clarity\": 1,\n",
      "                \"completeness\": 1,\n",
      "                \"grounding\": 1,\n",
      "                \"reasoning\": 1\n",
      "            },\n",
      "            \"total\": 1.0\n",
      "        }\n",
      "    }\n",
      "}\n",
      "__________________________________________________________\n",
      "{\n",
      "    \"deepseek-r1:8b\": {\n",
      "        \"r_0\": {\n",
      "            \"confidence_estimate\": 0.95,\n",
      "            \"justification\": \"The response is accurate, complete, grounded, reasoned, and clear, correctly handling the empty context without errors.\",\n",
      "            \"scores\": {\n",
      "                \"accuracy\": 5,\n",
      "                \"clarity\": 5,\n",
      "                \"completeness\": 5,\n",
      "                \"grounding\": 5,\n",
      "                \"reasoning\": 5\n",
      "            },\n",
      "            \"total\": 5.0\n",
      "        },\n",
      "        \"r_1\": {\n",
      "            \"confidence_estimate\": 0.9,\n",
      "            \"justification\": \"The response is factually accurate but lacks proper grounding in the context, with flawed reasoning for providing external information.\",\n",
      "            \"scores\": {\n",
      "                \"accuracy\": 5,\n",
      "                \"clarity\": 5,\n",
      "                \"completeness\": 5,\n",
      "                \"grounding\": 1,\n",
      "                \"reasoning\": 1\n",
      "            },\n",
      "            \"total\": 3.6\n",
      "        },\n",
      "        \"r_2\": {\n",
      "            \"confidence_estimate\": 0.9,\n",
      "            \"justification\": \"The response is accurate given the empty context but lacks completeness, grounding, and reasoning, resulting in a low overall score.\",\n",
      "            \"scores\": {\n",
      "                \"accuracy\": 5,\n",
      "                \"clarity\": 4,\n",
      "                \"completeness\": 1,\n",
      "                \"grounding\": 1,\n",
      "                \"reasoning\": 1\n",
      "            },\n",
      "            \"total\": 2.55\n",
      "        },\n",
      "        \"r_3\": {\n",
      "            \"confidence_estimate\": 0.9,\n",
      "            \"justification\": \"The response is accurate, complete, and clear, with adequate grounding and reasoning, though verification of the snippet reference is not possible without context.\",\n",
      "            \"scores\": {\n",
      "                \"accuracy\": 5,\n",
      "                \"clarity\": 5,\n",
      "                \"completeness\": 5,\n",
      "                \"grounding\": 4,\n",
      "                \"reasoning\": 4\n",
      "            },\n",
      "            \"total\": 4.65\n",
      "        },\n",
      "        \"r_4\": {\n",
      "            \"confidence_estimate\": 0.95,\n",
      "            \"justification\": \"The response accurately follows the prompt and context rules, addressing all parts with proper grounding and clear structure.\",\n",
      "            \"scores\": {\n",
      "                \"accuracy\": 5,\n",
      "                \"clarity\": 5,\n",
      "                \"completeness\": 5,\n",
      "                \"grounding\": 5,\n",
      "                \"reasoning\": 5\n",
      "            },\n",
      "            \"total\": 5.0\n",
      "        },\n",
      "        \"r_6\": {\n",
      "            \"confidence_estimate\": 0.95,\n",
      "            \"justification\": \"The candidate response accurately and fully addresses the prompt by correctly stating the answer is not in context, ensuring all elements are covered and reasoning is sound based on the absence of information.\",\n",
      "            \"scores\": {\n",
      "                \"accuracy\": 5,\n",
      "                \"clarity\": 5,\n",
      "                \"completeness\": 5,\n",
      "                \"grounding\": 5,\n",
      "                \"reasoning\": 5\n",
      "            },\n",
      "            \"total\": 5.0\n",
      "        }\n",
      "    },\n",
      "    \"gpt-oss:20b\": {\n",
      "        \"r_0\": {\n",
      "            \"confidence_estimate\": 0.99,\n",
      "            \"justification\": \"The response correctly follows the instruction to state \\\"Not in context\\\" when no relevant information is present, fully addressing the prompt with accurate, grounded, clear reasoning.\",\n",
      "            \"scores\": {\n",
      "                \"accuracy\": 5,\n",
      "                \"clarity\": 5,\n",
      "                \"completeness\": 5,\n",
      "                \"grounding\": 5,\n",
      "                \"reasoning\": 5\n",
      "            },\n",
      "            \"total\": 5.0\n",
      "        },\n",
      "        \"r_1\": {\n",
      "            \"confidence_estimate\": 0.85,\n",
      "            \"justification\": \"The answer is factually correct but it does not adhere to the instruction to rely only on context, lacks proper grounding, and the reasoning provided is logically inconsistent.\",\n",
      "            \"scores\": {\n",
      "                \"accuracy\": 5,\n",
      "                \"clarity\": 5,\n",
      "                \"completeness\": 5,\n",
      "                \"grounding\": 1,\n",
      "                \"reasoning\": 2\n",
      "            },\n",
      "            \"total\": 3.75\n",
      "        },\n",
      "        \"r_2\": {\n",
      "            \"confidence_estimate\": 0.95,\n",
      "            \"justification\": \"The response correctly follows the system instruction by stating 'Not in context' when the information is absent, thus it is accurate, complete, well-grounded, logically sound, and clear.\",\n",
      "            \"scores\": {\n",
      "                \"accuracy\": 5,\n",
      "                \"clarity\": 5,\n",
      "                \"completeness\": 5,\n",
      "                \"grounding\": 5,\n",
      "                \"reasoning\": 4\n",
      "            },\n",
      "            \"total\": 4.85\n",
      "        },\n",
      "        \"r_3\": {\n",
      "            \"confidence_estimate\": 0.8,\n",
      "            \"justification\": \"The answer is factually correct and fully addresses the question, but it incorrectly cites a non-existent context snippet, resulting in poor grounding and weak reasoning.\",\n",
      "            \"scores\": {\n",
      "                \"accuracy\": 5,\n",
      "                \"clarity\": 5,\n",
      "                \"completeness\": 5,\n",
      "                \"grounding\": 1,\n",
      "                \"reasoning\": 2\n",
      "            },\n",
      "            \"total\": 3.75\n",
      "        },\n",
      "        \"r_4\": {\n",
      "            \"confidence_estimate\": 0.98,\n",
      "            \"justification\": \"The candidate correctly follows the instruction to answer 'Not in context', provides a logical reasoning that the context contains no relevant information, and adheres to the required format with clear structure.\",\n",
      "            \"scores\": {\n",
      "                \"accuracy\": 5,\n",
      "                \"clarity\": 5,\n",
      "                \"completeness\": 5,\n",
      "                \"grounding\": 5,\n",
      "                \"reasoning\": 5\n",
      "            },\n",
      "            \"total\": 5.0\n",
      "        },\n",
      "        \"r_6\": {\n",
      "            \"confidence_estimate\": 0.95,\n",
      "            \"justification\": \"The response accurately follows the system instructions by stating \\\"Not in context.\\\" and provides all required sections with clear, concise formatting, thereby satisfying all rubric criteria.\",\n",
      "            \"scores\": {\n",
      "                \"accuracy\": 5,\n",
      "                \"clarity\": 5,\n",
      "                \"completeness\": 5,\n",
      "                \"grounding\": 5,\n",
      "                \"reasoning\": 5\n",
      "            },\n",
      "            \"total\": 5.0\n",
      "        }\n",
      "    },\n",
      "    \"minstral-3\": {\n",
      "        \"r_0\": {\n",
      "            \"confidence_estimate\": 1.0,\n",
      "            \"justification\": \"The response perfectly adhered to the retrieval-grounded instructions, correctly identified the absence of relevant context, and provided a concise, well-structured answer with no factual errors or unsupported claims.\",\n",
      "            \"scores\": {\n",
      "                \"accuracy\": 5,\n",
      "                \"clarity\": 5,\n",
      "                \"completeness\": 5,\n",
      "                \"grounding\": 5,\n",
      "                \"reasoning\": 5\n",
      "            },\n",
      "            \"total\": 5.0\n",
      "        },\n",
      "        \"r_1\": {\n",
      "            \"confidence_estimate\": 0.95,\n",
      "            \"justification\": \"The response provided an accurate fact (Junko Tabei was the first female mountaineer to climb Everest) but failed to adhere to the instruction to ground answers solely in the provided context, instead inventing reasoning and citing no snippets.\",\n",
      "            \"scores\": {\n",
      "                \"accuracy\": 3,\n",
      "                \"clarity\": 4,\n",
      "                \"completeness\": 2,\n",
      "                \"grounding\": 1,\n",
      "                \"reasoning\": 2\n",
      "            },\n",
      "            \"total\": 2.25\n",
      "        },\n",
      "        \"r_2\": {\n",
      "            \"confidence_estimate\": 1.0,\n",
      "            \"justification\": \"The response correctly adheres to the instructions by explicitly stating 'Not in context' when no relevant information was provided, demonstrating full accuracy, completeness, grounding, logical coherence, and clarity.\",\n",
      "            \"scores\": {\n",
      "                \"accuracy\": 5,\n",
      "                \"clarity\": 5,\n",
      "                \"completeness\": 5,\n",
      "                \"grounding\": 5,\n",
      "                \"reasoning\": 5\n",
      "            },\n",
      "            \"total\": 5.0\n",
      "        },\n",
      "        \"r_3\": {\n",
      "            \"confidence_estimate\": 1.0,\n",
      "            \"justification\": \"The response is factually incorrect because the context provided is empty, making any claim unsupported; the answer violates the retrieval-grounded instruction by inventing a fact.\",\n",
      "            \"scores\": {\n",
      "                \"accuracy\": 1,\n",
      "                \"clarity\": 2,\n",
      "                \"completeness\": 1,\n",
      "                \"grounding\": 1,\n",
      "                \"reasoning\": 1\n",
      "            },\n",
      "            \"total\": 1.05\n",
      "        },\n",
      "        \"r_4\": {\n",
      "            \"confidence_estimate\": 1.0,\n",
      "            \"justification\": \"The response perfectly adheres to the instructions by explicitly stating 'Not in context' and providing a clear, concise justification tied to the absence of relevant information in the provided context, with no deviations or unsupported claims.\",\n",
      "            \"scores\": {\n",
      "                \"accuracy\": 5,\n",
      "                \"clarity\": 5,\n",
      "                \"completeness\": 5,\n",
      "                \"grounding\": 5,\n",
      "                \"reasoning\": 5\n",
      "            },\n",
      "            \"total\": 5.0\n",
      "        },\n",
      "        \"r_6\": {\n",
      "            \"confidence_estimate\": 1.0,\n",
      "            \"justification\": \"The response perfectly adhered to the instructions by explicitly stating 'Not in context' and providing a clear, concise justification with no unsupported claims or deviations.\",\n",
      "            \"scores\": {\n",
      "                \"accuracy\": 5,\n",
      "                \"clarity\": 5,\n",
      "                \"completeness\": 5,\n",
      "                \"grounding\": 5,\n",
      "                \"reasoning\": 5\n",
      "            },\n",
      "            \"total\": 5.0\n",
      "        }\n",
      "    },\n",
      "    \"mistral:7b\": {\n",
      "        \"r_0\": {\n",
      "            \"confidence_estimate\": 1,\n",
      "            \"justification\": \"The response accurately states that the provided context does not contain information about the first female mountaineer to climb Mount Everest. It fully addresses all parts of the prompt and provides a clear and well-structured answer, with no logical errors or unsupported claims.\",\n",
      "            \"scores\": {\n",
      "                \"accuracy\": 5,\n",
      "                \"clarity\": 5,\n",
      "                \"completeness\": 5,\n",
      "                \"grounding\": 5,\n",
      "                \"reasoning\": 5\n",
      "            },\n",
      "            \"total\": 5.0\n",
      "        },\n",
      "        \"r_1\": {\n",
      "            \"confidence_estimate\": 0.9,\n",
      "            \"justification\": \"The response accurately states that Junko Tabei was the first female mountaineer to climb Mount Everest, despite not finding the information in the context. However, the reasoning provided could be more straightforward and clearer. The reference to external sources is also missing but the given response does not include unsupported claims.\",\n",
      "            \"scores\": {\n",
      "                \"accuracy\": 5,\n",
      "                \"clarity\": 5,\n",
      "                \"completeness\": 5,\n",
      "                \"grounding\": 4,\n",
      "                \"reasoning\": 3\n",
      "            },\n",
      "            \"total\": 4.5\n",
      "        },\n",
      "        \"r_2\": {\n",
      "            \"confidence_estimate\": 1.0,\n",
      "            \"justification\": \"The response accurately states that the information about who was the first female mountaineer to climb Mount Everest is not in the provided context.\",\n",
      "            \"scores\": {\n",
      "                \"accuracy\": 5,\n",
      "                \"clarity\": 5,\n",
      "                \"completeness\": 5,\n",
      "                \"grounding\": 5,\n",
      "                \"reasoning\": 5\n",
      "            },\n",
      "            \"total\": 5.0\n",
      "        },\n",
      "        \"r_3\": {\n",
      "            \"confidence_estimate\": 1.0,\n",
      "            \"justification\": \"The candidate's response is factually accurate, fully addresses all parts of the prompt, provides clear references to the context, demonstrates sound reasoning and exhibits a high level of clarity.\",\n",
      "            \"scores\": {\n",
      "                \"accuracy\": 5,\n",
      "                \"clarity\": 5,\n",
      "                \"completeness\": 5,\n",
      "                \"grounding\": 5,\n",
      "                \"reasoning\": 5\n",
      "            },\n",
      "            \"total\": 5.0\n",
      "        },\n",
      "        \"r_4\": {\n",
      "            \"confidence_estimate\": 1.0,\n",
      "            \"justification\": \"The response is accurate as it correctly identifies that the provided context does not contain information about the first female mountaineer to climb Mount Everest. The response fully addresses all parts of the prompt, and provides clear reasoning and well-structured content.\",\n",
      "            \"scores\": {\n",
      "                \"accuracy\": 5,\n",
      "                \"clarity\": 5,\n",
      "                \"completeness\": 5,\n",
      "                \"grounding\": 5,\n",
      "                \"reasoning\": 5\n",
      "            },\n",
      "            \"total\": 5.0\n",
      "        },\n",
      "        \"r_6\": {\n",
      "            \"confidence_estimate\": 1.0,\n",
      "            \"justification\": \"The response is factually correct as it correctly identifies that the context does not contain information about the first female mountaineer to climb Mount Everest. It addresses all parts of the prompt and provides a clear and concise answer without any unsupported claims or flawed reasoning.\",\n",
      "            \"scores\": {\n",
      "                \"accuracy\": 5,\n",
      "                \"clarity\": 5,\n",
      "                \"completeness\": 5,\n",
      "                \"grounding\": 5,\n",
      "                \"reasoning\": 5\n",
      "            },\n",
      "            \"total\": 5.0\n",
      "        }\n",
      "    },\n",
      "    \"phi3:mini\": {\n",
      "        \"r_0\": {\n",
      "            \"confidence_estimate\": 0.0,\n",
      "            \"justification\": 0,\n",
      "            \"scores\": {\n",
      "                \"accuracy\": 0,\n",
      "                \"clarity\": 0,\n",
      "                \"completeness\": 0,\n",
      "                \"grounding\": 0,\n",
      "                \"reasoning\": 0\n",
      "            },\n",
      "            \"total\": 0.0\n",
      "        },\n",
      "        \"r_1\": {\n",
      "            \"confidence_estimate\": 0.75,\n",
      "            \"justification\": \"The response accurately identifies Junko Tabei as a notable figure in mountaineering and her achievement of climbing Everest aligns with historical facts; however, the reasoning is flawed due to lack of direct evidence from context provided, grounding lacks citation or source reference.\",\n",
      "            \"scores\": {\n",
      "                \"accuracy\": 5,\n",
      "                \"clarity\": 5,\n",
      "                \"completeness\": 4,\n",
      "                \"grounding\": 1,\n",
      "                \"reasoning\": 2\n",
      "            },\n",
      "            \"total\": 5.25\n",
      "        },\n",
      "        \"r_2\": {\n",
      "            \"confidence_estimate\": 0,\n",
      "            \"justification\": 0,\n",
      "            \"scores\": {\n",
      "                \"accuracy\": 0,\n",
      "                \"clarity\": 0,\n",
      "                \"completeness\": 0,\n",
      "                \"grounding\": 0,\n",
      "                \"reasoning\": 0\n",
      "            },\n",
      "            \"total\": 0.0\n",
      "        },\n",
      "        \"r_3\": {\n",
      "            \"confidence_estimate\": 0.95,\n",
      "            \"justification\": \"The response accurately identifies Junko Tabei as the first female to climb Everest but fails in providing proper grounding for its claim.\",\n",
      "            \"scores\": {\n",
      "                \"accuracy\": 5,\n",
      "                \"clarity\": 5,\n",
      "                \"completeness\": 2,\n",
      "                \"grounding\": 1,\n",
      "                \"reasoning\": 5\n",
      "            },\n",
      "            \"total\": 5.175000000000001\n",
      "        },\n",
      "        \"r_4\": {\n",
      "            \"confidence_estimate\": 0.95,\n",
      "            \"justification\": \"The candidate response accurately identifies that the information is not in context and explains why it cannot answer based on provided texts.\",\n",
      "            \"scores\": {\n",
      "                \"accuracy\": 0,\n",
      "                \"clarity\": 0,\n",
      "                \"completeness\": 0,\n",
      "                \"grounding\": 0,\n",
      "                \"reasoning\": 0\n",
      "            },\n",
      "            \"total\": 0.0\n",
      "        },\n",
      "        \"r_6\": {\n",
      "            \"confidence_estimate\": 0.0,\n",
      "            \"justification\": 0,\n",
      "            \"scores\": {\n",
      "                \"accuracy\": 0,\n",
      "                \"clarity\": 0,\n",
      "                \"completeness\": 0,\n",
      "                \"grounding\": 0,\n",
      "                \"reasoning\": 0\n",
      "            },\n",
      "            \"total\": 0.0\n",
      "        }\n",
      "    },\n",
      "    \"starling-1m\": {\n",
      "        \"r_0\": {\n",
      "            \"confidence_estimate\": 0.8,\n",
      "            \"justification\": \"The response correctly identifies that the context provided does not contain information about female mountaineers or Mount Everest climbers. However, it could have mentioned that the answer to the question (Edna Brightwell) is found in external sources and not within the context itself, as the prompt suggests. The response shows clarity and conciseness but misses the opportunity to provide additional relevant information. Grounding was affected by not referencing external sources.\",\n",
      "            \"scores\": {\n",
      "                \"accuracy\": 5,\n",
      "                \"clarity\": 5,\n",
      "                \"completeness\": 3,\n",
      "                \"grounding\": 2,\n",
      "                \"reasoning\": 4\n",
      "            },\n",
      "            \"total\": 3.75\n",
      "        },\n",
      "        \"r_1\": {\n",
      "            \"confidence_estimate\": 0.98,\n",
      "            \"justification\": \"The response accurately identifies Junko Tabei as the first female mountaineer to climb Mount Everest and provides the correct date of her achievement. However, the grounding score is reduced by one point because the response mentions that the information about Junko Tabei was not included in the context, which is incorrect; the context merely prohibits making statements without evidence, rather than stating that there's no relevant information available.\",\n",
      "            \"scores\": {\n",
      "                \"accuracy\": 5,\n",
      "                \"clarity\": 5,\n",
      "                \"completeness\": 5,\n",
      "                \"grounding\": 4,\n",
      "                \"reasoning\": 5\n",
      "            },\n",
      "            \"total\": 4.8\n",
      "        },\n",
      "        \"r_2\": {\n",
      "            \"confidence_estimate\": 0.25,\n",
      "            \"justification\": \"The response does not answer the user's question about the first female mountaineer to climb Mount Everest. It simply states 'Not in context,' which is accurate but does not provide any relevant information or historical fact. Additionally, the response lacks any evidence or reasoning that could be checked against a verifiable source. The response is clear and concise, but it misses the core part of the prompt, resulting in low scores across all categories.\",\n",
      "            \"scores\": {\n",
      "                \"accuracy\": 1,\n",
      "                \"clarity\": 1,\n",
      "                \"completeness\": 1,\n",
      "                \"grounding\": 1,\n",
      "                \"reasoning\": 1\n",
      "            },\n",
      "            \"total\": 1.0\n",
      "        },\n",
      "        \"r_3\": {\n",
      "            \"$defs\": {\n",
      "                \"Scores\": {\n",
      "                    \"properties\": {\n",
      "                        \"accuracy\": {\n",
      "                            \"title\": \"Accuracy\",\n",
      "                            \"type\": \"integer\"\n",
      "                        },\n",
      "                        \"clarity\": {\n",
      "                            \"title\": \"Clarity\",\n",
      "                            \"type\": \"integer\"\n",
      "                        },\n",
      "                        \"completeness\": {\n",
      "                            \"title\": \"Completeness\",\n",
      "                            \"type\": \"integer\"\n",
      "                        },\n",
      "                        \"grounding\": {\n",
      "                            \"title\": \"Grounding\",\n",
      "                            \"type\": \"integer\"\n",
      "                        },\n",
      "                        \"reasoning\": {\n",
      "                            \"title\": \"Reasoning\",\n",
      "                            \"type\": \"integer\"\n",
      "                        }\n",
      "                    },\n",
      "                    \"required\": [\n",
      "                        \"accuracy\",\n",
      "                        \"completeness\",\n",
      "                        \"grounding\",\n",
      "                        \"reasoning\",\n",
      "                        \"clarity\"\n",
      "                    ],\n",
      "                    \"title\": \"Scores\",\n",
      "                    \"type\": \"object\"\n",
      "                },\n",
      "                \"properties\": {\n",
      "                    \"confidence_estimate\": {\n",
      "                        \"title\": \"Confidence Estimate\",\n",
      "                        \"type\": \"number\"\n",
      "                    },\n",
      "                    \"justification\": {\n",
      "                        \"title\": \"Justification\",\n",
      "                        \"type\": \"string\"\n",
      "                    },\n",
      "                    \"required\": [\n",
      "                        \"scores\",\n",
      "                        \"confidence_estimate\",\n",
      "                        \"justification\"\n",
      "                    ],\n",
      "                    \"scores\": {\n",
      "                        \"$ref\": \"#/$defs/Scores\"\n",
      "                    }\n",
      "                }\n",
      "            },\n",
      "            \"confidence_estimate\": 0.98,\n",
      "            \"justification\": \"The candidate response correctly identifies Junko Tabei as the first female mountaineer to climb Mount Everest and cites the context (snippet 2) to support this fact. The response is concise and well-structured, with clear information that addresses all parts of the prompt. It demonstrates accurate information based on known facts, full coverage of the topic, strong grounding in the provided context, sound reasoning, and clear communication.\",\n",
      "            \"scores\": {\n",
      "                \"accuracy\": 5,\n",
      "                \"clarity\": 5,\n",
      "                \"completeness\": 5,\n",
      "                \"grounding\": 5,\n",
      "                \"reasoning\": 5\n",
      "            },\n",
      "            \"total\": 5.0\n",
      "        },\n",
      "        \"r_4\": {\n",
      "            \"confidence_estimate\": 0.75,\n",
      "            \"justification\": \"The response correctly identifies that the given context does not provide information about the first female mountaineer to climb Mount Everest. However, it lacks grounding as it fails to cite any snippets or reference verifiable sources. The reasoning and completeness are somewhat satisfactory as the answer acknowledges the absence of such information in the provided context. Clarity is good as the response is concise and well-structured.\",\n",
      "            \"scores\": {\n",
      "                \"accuracy\": 3,\n",
      "                \"clarity\": 4,\n",
      "                \"completeness\": 2,\n",
      "                \"grounding\": 1,\n",
      "                \"reasoning\": 3\n",
      "            },\n",
      "            \"total\": 2.4\n",
      "        },\n",
      "        \"r_6\": {\n",
      "            \"confidence_estimate\": 0.25,\n",
      "            \"justification\": \"The response does not provide any information about the first female mountaineer to climb Mount Everest and instead states that the context is 'Not in context.' The answer lacks factual accuracy, fails to address any part of the prompt, offers no grounding or citation of relevant information, displays flawed reasoning by stating there's no relevant information available, and does not provide a clear or concise response. Due to these reasons, it scores poorly on all criteria.\",\n",
      "            \"scores\": {\n",
      "                \"accuracy\": 1,\n",
      "                \"clarity\": 1,\n",
      "                \"completeness\": 1,\n",
      "                \"grounding\": 1,\n",
      "                \"reasoning\": 1\n",
      "            },\n",
      "            \"total\": 1.0\n",
      "        }\n",
      "    }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "pretty_print_json(scoring_matrix)\n",
    "print(\"__________________________________________________________\")\n",
    "for k in audit_report['normalization']:\n",
    "    for j in scoring_matrix[k]:\n",
    "        scoring_matrix[k][j]['total'] *= audit_report['normalization'][k]\n",
    "pretty_print_json(scoring_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6330b0c8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv_gaim",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
