{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d056d7fb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(OllamaLLM(model='deepseek-r1:8b'), 'deepseek-r1:8b'),\n",
       " (OllamaLLM(model='starling-lm'), 'starling-1m'),\n",
       " (OllamaLLM(model='ministral-3'), 'minstral-3'),\n",
       " (OllamaLLM(model='mistral:7b'), 'mistral:7b'),\n",
       " (OllamaLLM(model='phi3:mini'), 'phi3:mini'),\n",
       " (OllamaLLM(model='gemma2:9b'), 'gemma2:9b'),\n",
       " (OllamaLLM(model='gpt-oss:20b'), 'gpt-oss:20b')]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from ai_council.council import *\n",
    "from ai_council.prompts import *\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import PydanticOutputParser\n",
    "import ast\n",
    "\n",
    "[(k[\"llm\"] , k[\"name\"]) for k in MODELS]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ac38de6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# responses = []\n",
    "# for i,k in enumerate(MODELS):\n",
    "#     if k['id'] == 'evaluator':\n",
    "#         continue\n",
    "#     print(f\"Response by {k[\"name\"]}\")\n",
    "#     text = k[\"llm\"].invoke('Tell me a joke')\n",
    "#     print(text)\n",
    "#     print(\"____________________________________________________\")\n",
    "#     responses.append(\n",
    "#         {\n",
    "#             \"response_id\" : f\"r_{i}\",\n",
    "#             \"model_id\" : k[\"id\"],\n",
    "#             \"text\" : text\n",
    "#         }\n",
    "#     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0658e163",
   "metadata": {},
   "outputs": [],
   "source": [
    "# scorer_parser = PydanticOutputParser(pydantic_object=scoring_output)\n",
    "# scoring_prompt = ChatPromptTemplate.from_template(scoring_template)\n",
    "# chains = [(scoring_prompt|k[\"llm\"], k) for k in MODELS]\n",
    "# scoring_matrix = {}\n",
    "# for chain in chains:\n",
    "#     temp_dict = {}\n",
    "#     print(chain[1]['name'])\n",
    "#     for i in range(len(responses)):\n",
    "#         result = chain[0].invoke({\"user_prompt\" : \"Tell me a joke\", \"candidate_response\" : responses[i], \"output_format\" : scorer_parser.get_format_instructions()})\n",
    "#         print(result)\n",
    "#         json_response = ast.literal_eval('{' + extract_first_curly_balanced(result) + '}')\n",
    "#         json_response['total'] = sum([WEIGHTS[k] * json_response['scores'][k] for k in WEIGHTS])\n",
    "#         temp_dict[responses[i][\"response_id\"]] = json_response\n",
    "#     scoring_matrix[chain[1]['name']] = temp_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "912d9606",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response from deepseek-r1:8b\n",
      "Response generated by deepseek-r1:8b with id r_0\n",
      "Response from starling-1m\n",
      "Response generated by starling-1m with id r_1\n",
      "Response from minstral-3\n",
      "Response generated by minstral-3 with id r_2\n",
      "Response from mistral:7b\n",
      "Response generated by mistral:7b with id r_3\n",
      "Response from phi3:mini\n",
      "Response generated by phi3:mini with id r_4\n",
      "Response from gpt-oss:20b\n",
      "Response generated by gpt-oss:20b with id r_6\n",
      "{'response_id': 'r_0', 'model_id': 'expert_1', 'text': '**Final answer:**\\nNot in context.\\n\\n**Brief reasoning:**\\nThe context provided does not contain any information about female mountaineers or specific records related to the first ascent of Mount Everest.\\n\\n**Snippets used:**\\nnone'}\n",
      "{'response_id': 'r_1', 'model_id': 'expert_2', 'text': '1. Junko Tabei was the first female mountaineer to climb Mount Everest.\\n2. She achieved this feat on May 16, 1975, as part of an all-women Japanese expedition.\\n3. Snippet number used: Not applicable (information not provided in the context).'}\n",
      "{'response_id': 'r_2', 'model_id': 'expert_3', 'text': 'Not in context.'}\n",
      "{'response_id': 'r_3', 'model_id': 'expert_4', 'text': \" 1. Tenzing Norgay Sherpa was the first female mountaineer to climb Mount Everest. [This statement is not explicitly mentioned in the provided context, but it's a common knowledge that Tenzing Norgay Sherpa climbed Mount Everest with Sir Edmund Hillary and Junko Tabei is the first woman to ascend Mount Everest from the Tibetan side.]\\n2. The given context does not provide specific details about the first female climber of Mount Everest, but it's widely known that Tenzing Norgay Sherpa was the first person to climb Mount Everest with Sir Edmund Hillary in 1953. Junko Tabei, a Japanese mountaineer, is recognized as the first woman to ascend Mount Everest from the Tibetan side in 1975. [None]\"}\n",
      "{'response_id': 'r_4', 'model_id': 'expert_5', 'text': \"1. Not in context. (The information provided does not contain details about any female mountaineer's first ascent of Mount Everest.)\\n2. Therefore, we cannot determine who was the first female to climb Mount Everest based on this document alone.\\n3. #\"}\n",
      "{'response_id': 'r_6', 'model_id': 'expert_6', 'text': '1. Not in context.  \\n2. No relevant information was provided in the context.  \\n3. none'}\n"
     ]
    }
   ],
   "source": [
    "responses , user_prompt = generate_expert_response(\"Who was the first female mountaineer to climb Mount Everest\", \"\")\n",
    "for response in responses:\n",
    "    print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "49996425",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "deepseek-r1:8b\n",
      "Scoring complete for r_0 by deepseek-r1:8b\n",
      "Scoring complete for r_1 by deepseek-r1:8b\n",
      "Scoring complete for r_2 by deepseek-r1:8b\n",
      "Scoring complete for r_3 by deepseek-r1:8b\n",
      "Scoring complete for r_4 by deepseek-r1:8b\n",
      "Scoring complete for r_6 by deepseek-r1:8b\n",
      "starling-1m\n",
      "Scoring complete for r_0 by starling-1m\n",
      "Scoring complete for r_1 by starling-1m\n",
      "Scoring complete for r_2 by starling-1m\n",
      "Scoring complete for r_3 by starling-1m\n",
      "Could not parse r_3 by starling-1m due to expected string or bytes-like object, got 'NoneType'\n",
      "Scoring complete for r_4 by starling-1m\n",
      "Scoring complete for r_6 by starling-1m\n",
      "minstral-3\n",
      "Scoring complete for r_0 by minstral-3\n",
      "Scoring complete for r_1 by minstral-3\n",
      "Scoring complete for r_2 by minstral-3\n",
      "Scoring complete for r_3 by minstral-3\n",
      "Scoring complete for r_4 by minstral-3\n",
      "Scoring complete for r_6 by minstral-3\n",
      "mistral:7b\n",
      "Scoring complete for r_0 by mistral:7b\n",
      "Scoring complete for r_1 by mistral:7b\n",
      "Scoring complete for r_2 by mistral:7b\n",
      "Scoring complete for r_3 by mistral:7b\n",
      "Scoring complete for r_4 by mistral:7b\n",
      "Scoring complete for r_6 by mistral:7b\n",
      "phi3:mini\n",
      "Scoring complete for r_0 by phi3:mini\n",
      "Scoring complete for r_1 by phi3:mini\n",
      "Scoring complete for r_2 by phi3:mini\n",
      "Scoring complete for r_3 by phi3:mini\n",
      "Scoring complete for r_4 by phi3:mini\n",
      "Scoring complete for r_6 by phi3:mini\n",
      "gpt-oss:20b\n",
      "Scoring complete for r_0 by gpt-oss:20b\n",
      "Scoring complete for r_1 by gpt-oss:20b\n",
      "Scoring complete for r_2 by gpt-oss:20b\n",
      "Scoring complete for r_3 by gpt-oss:20b\n",
      "Scoring complete for r_4 by gpt-oss:20b\n",
      "Scoring complete for r_6 by gpt-oss:20b\n"
     ]
    }
   ],
   "source": [
    "scoring_matrix = generate_scores(responses, user_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1a2e635e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptValue(messages=[HumanMessage(content='\\n    SYSTEM:\\n    You are a retrieval-grounded assistant. Use only the information in the CONTEXT. \\n    If the answer is not in the context, say: \"Not in context.\" \\n    Do not guess or invent facts.\\n\\n    FORMAT:\\n    1. Final answer (1-2 lines)\\n    2. Brief reasoning (1-2 lines)\\n    3. Snippets used (# or \"none\")\\n\\n    USER:\\n    Who was the first female mountaineer to climb Mount Everest\\n\\n    CONTEXT:\\n    \\n\\n    RULES:\\n    - Base all statements strictly on the context.\\n    - Cite snippet numbers when used.\\n    - Keep responses short and precise.\\n', additional_kwargs={}, response_metadata={})])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "user_prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c565d115",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import re\n",
    "# ast.literal_eval('{'+re.sub(r'//.*','',extract_first_curly_balanced(scoring_results[-1])).replace('null' , '0') + '}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "171f904d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"deepseek-r1:8b\": {\n",
      "        \"r_0\": {\n",
      "            \"confidence_estimate\": 0.99,\n",
      "            \"justification\": \"The response accurately and completely addresses the prompt with proper grounding due to the empty context.\",\n",
      "            \"scores\": {\n",
      "                \"accuracy\": 5,\n",
      "                \"clarity\": 5,\n",
      "                \"completeness\": 5,\n",
      "                \"grounding\": 5,\n",
      "                \"reasoning\": 5\n",
      "            },\n",
      "            \"total\": 5.0\n",
      "        },\n",
      "        \"r_1\": {\n",
      "            \"confidence_estimate\": 0.9,\n",
      "            \"justification\": \"The response is factually correct and complete but lacks grounding due to the absence of context.\",\n",
      "            \"scores\": {\n",
      "                \"accuracy\": 5,\n",
      "                \"clarity\": 5,\n",
      "                \"completeness\": 5,\n",
      "                \"grounding\": 1,\n",
      "                \"reasoning\": 4\n",
      "            },\n",
      "            \"total\": 4.05\n",
      "        },\n",
      "        \"r_2\": {\n",
      "            \"confidence_estimate\": 0.9,\n",
      "            \"justification\": \"The response is accurate and clear but incomplete and lacks coherent reasoning and proper grounding.\",\n",
      "            \"scores\": {\n",
      "                \"accuracy\": 5,\n",
      "                \"clarity\": 4,\n",
      "                \"completeness\": 1,\n",
      "                \"grounding\": 3,\n",
      "                \"reasoning\": 1\n",
      "            },\n",
      "            \"total\": 2.95\n",
      "        },\n",
      "        \"r_3\": {\n",
      "            \"confidence_estimate\": 0.9,\n",
      "            \"justification\": \"The response contains factual errors, such as claiming Tenzing Norgay Sherpa was female, and does not properly use or reference the empty context, leading to unsupported claims.\",\n",
      "            \"scores\": {\n",
      "                \"accuracy\": 1,\n",
      "                \"clarity\": 2,\n",
      "                \"completeness\": 1,\n",
      "                \"grounding\": 1,\n",
      "                \"reasoning\": 1\n",
      "            },\n",
      "            \"total\": 1.05\n",
      "        },\n",
      "        \"r_4\": {\n",
      "            \"confidence_estimate\": 0.9,\n",
      "            \"justification\": \"The response accurately adheres to the context rules and provides a complete, grounded, and clear response given the empty context.\",\n",
      "            \"scores\": {\n",
      "                \"accuracy\": 5,\n",
      "                \"clarity\": 5,\n",
      "                \"completeness\": 5,\n",
      "                \"grounding\": 5,\n",
      "                \"reasoning\": 5\n",
      "            },\n",
      "            \"total\": 5.0\n",
      "        },\n",
      "        \"r_6\": {\n",
      "            \"confidence_estimate\": 0.99,\n",
      "            \"justification\": \"The response accurately addresses the empty context by correctly stating the information is not present, fully covering all prompt parts with proper grounding and clear structure.\",\n",
      "            \"scores\": {\n",
      "                \"accuracy\": 5,\n",
      "                \"clarity\": 5,\n",
      "                \"completeness\": 5,\n",
      "                \"grounding\": 5,\n",
      "                \"reasoning\": 5\n",
      "            },\n",
      "            \"total\": 5.0\n",
      "        }\n",
      "    },\n",
      "    \"gpt-oss:20b\": {\n",
      "        \"r_0\": {\n",
      "            \"confidence_estimate\": 0.95,\n",
      "            \"justification\": \"The candidate correctly follows the instruction to reply 'Not in context' when no information is available, providing accurate, complete, and well-grounded reasoning with clear presentation.\",\n",
      "            \"scores\": {\n",
      "                \"accuracy\": 5,\n",
      "                \"clarity\": 5,\n",
      "                \"completeness\": 5,\n",
      "                \"grounding\": 5,\n",
      "                \"reasoning\": 5\n",
      "            },\n",
      "            \"total\": 5.0\n",
      "        },\n",
      "        \"r_1\": {\n",
      "            \"confidence_estimate\": 0.9,\n",
      "            \"justification\": \"The answer correctly identifies Junko Tabei and provides relevant details, but it is not grounded in the provided context and offers no evidence or reasoning steps, resulting in a low grounding score.\",\n",
      "            \"scores\": {\n",
      "                \"accuracy\": 5,\n",
      "                \"clarity\": 5,\n",
      "                \"completeness\": 5,\n",
      "                \"grounding\": 1,\n",
      "                \"reasoning\": 1\n",
      "            },\n",
      "            \"total\": 3.6\n",
      "        },\n",
      "        \"r_2\": {\n",
      "            \"confidence_estimate\": 0.99,\n",
      "            \"justification\": \"The answer correctly follows the instruction to state \\\"Not in context.\\\" when the context provides no information, covering all required aspects and exhibiting clear, concise reasoning.\",\n",
      "            \"scores\": {\n",
      "                \"accuracy\": 5,\n",
      "                \"clarity\": 5,\n",
      "                \"completeness\": 5,\n",
      "                \"grounding\": 5,\n",
      "                \"reasoning\": 5\n",
      "            },\n",
      "            \"total\": 5.0\n",
      "        },\n",
      "        \"r_3\": {\n",
      "            \"confidence_estimate\": 0.9,\n",
      "            \"justification\": \"The response contains factual errors, does not follow the instruction to rely solely on the provided context, and provides unsupported claims.\",\n",
      "            \"scores\": {\n",
      "                \"accuracy\": 1,\n",
      "                \"clarity\": 2,\n",
      "                \"completeness\": 2,\n",
      "                \"grounding\": 1,\n",
      "                \"reasoning\": 1\n",
      "            },\n",
      "            \"total\": 1.3\n",
      "        },\n",
      "        \"r_4\": {\n",
      "            \"confidence_estimate\": 0.95,\n",
      "            \"justification\": \"The answer correctly acknowledges the lack of relevant context, follows the instructions, and provides coherent reasoning, though it does not reference any specific snippet, so grounding is modest.\",\n",
      "            \"scores\": {\n",
      "                \"accuracy\": 5,\n",
      "                \"clarity\": 5,\n",
      "                \"completeness\": 5,\n",
      "                \"grounding\": 3,\n",
      "                \"reasoning\": 5\n",
      "            },\n",
      "            \"total\": 4.6\n",
      "        },\n",
      "        \"r_6\": {\n",
      "            \"confidence_estimate\": 0.95,\n",
      "            \"justification\": \"The response correctly follows the instruction to say 'Not in context' and includes all required sections with accurate reasoning.\",\n",
      "            \"scores\": {\n",
      "                \"accuracy\": 5,\n",
      "                \"clarity\": 5,\n",
      "                \"completeness\": 5,\n",
      "                \"grounding\": 5,\n",
      "                \"reasoning\": 5\n",
      "            },\n",
      "            \"total\": 5.0\n",
      "        }\n",
      "    },\n",
      "    \"minstral-3\": {\n",
      "        \"r_0\": {\n",
      "            \"confidence_estimate\": 1.0,\n",
      "            \"justification\": \"The response perfectly adhered to the instructions by correctly identifying the absence of relevant context and providing a concise, well-supported explanation without deviation or unsupported claims.\",\n",
      "            \"scores\": {\n",
      "                \"accuracy\": 5,\n",
      "                \"clarity\": 5,\n",
      "                \"completeness\": 5,\n",
      "                \"grounding\": 5,\n",
      "                \"reasoning\": 5\n",
      "            },\n",
      "            \"total\": 5.0\n",
      "        },\n",
      "        \"r_1\": {\n",
      "            \"confidence_estimate\": 1.0,\n",
      "            \"justification\": \"The response is factually correct about Junko Tabei being the first female mountaineer to climb Everest, but it fails to adhere to the instruction to ground answers solely in the provided context (which was empty).\",\n",
      "            \"scores\": {\n",
      "                \"accuracy\": 5,\n",
      "                \"clarity\": 5,\n",
      "                \"completeness\": 3,\n",
      "                \"grounding\": 1,\n",
      "                \"reasoning\": 1\n",
      "            },\n",
      "            \"total\": 3.1\n",
      "        },\n",
      "        \"r_2\": {\n",
      "            \"confidence_estimate\": 1.0,\n",
      "            \"justification\": \"The response perfectly adhered to the instructions by explicitly stating 'Not in context' when no relevant information was provided, demonstrating full accuracy, completeness, grounding, sound reasoning, and clarity.\",\n",
      "            \"scores\": {\n",
      "                \"accuracy\": 5,\n",
      "                \"clarity\": 5,\n",
      "                \"completeness\": 5,\n",
      "                \"grounding\": 5,\n",
      "                \"reasoning\": 5\n",
      "            },\n",
      "            \"total\": 5.0\n",
      "        },\n",
      "        \"r_3\": {\n",
      "            \"confidence_estimate\": 0.9,\n",
      "            \"justification\": \"The response incorrectly states Tenzing Norgay Sherpa was a female mountaineer and fails to ground the correct fact (Junko Tabei) in the provided context, which is empty; it also mixes unrelated historical details without proper citation.\",\n",
      "            \"scores\": {\n",
      "                \"accuracy\": 2,\n",
      "                \"clarity\": 3,\n",
      "                \"completeness\": 2,\n",
      "                \"grounding\": 1,\n",
      "                \"reasoning\": 2\n",
      "            },\n",
      "            \"total\": 1.85\n",
      "        },\n",
      "        \"r_4\": {\n",
      "            \"confidence_estimate\": 1.0,\n",
      "            \"justification\": \"The response perfectly adheres to the instructions by correctly stating 'Not in context' and providing a concise, accurate explanation referencing the empty context, with no deviations or unsupported claims.\",\n",
      "            \"scores\": {\n",
      "                \"accuracy\": 5,\n",
      "                \"clarity\": 5,\n",
      "                \"completeness\": 5,\n",
      "                \"grounding\": 5,\n",
      "                \"reasoning\": 5\n",
      "            },\n",
      "            \"total\": 5.0\n",
      "        },\n",
      "        \"r_6\": {\n",
      "            \"confidence_estimate\": 1.0,\n",
      "            \"justification\": \"The response perfectly adhered to the instructions by explicitly stating 'Not in context' and providing a concise, accurate justification with no unsupported claims or deviations from the given format.\",\n",
      "            \"scores\": {\n",
      "                \"accuracy\": 5,\n",
      "                \"clarity\": 5,\n",
      "                \"completeness\": 5,\n",
      "                \"grounding\": 5,\n",
      "                \"reasoning\": 5\n",
      "            },\n",
      "            \"total\": 5.0\n",
      "        }\n",
      "    },\n",
      "    \"mistral:7b\": {\n",
      "        \"r_0\": {\n",
      "            \"confidence_estimate\": 1.0,\n",
      "            \"justification\": \"The response accurately states that the context does not contain information about the first female mountaineer to climb Mount Everest. It completes all parts of the prompt and provides a clear, concise, well-structured answer with no factual errors or unsupported claims.\",\n",
      "            \"scores\": {\n",
      "                \"accuracy\": 5,\n",
      "                \"clarity\": 5,\n",
      "                \"completeness\": 5,\n",
      "                \"grounding\": 5,\n",
      "                \"reasoning\": 5\n",
      "            },\n",
      "            \"total\": 5.0\n",
      "        },\n",
      "        \"r_1\": {\n",
      "            \"confidence_estimate\": 0.95,\n",
      "            \"justification\": \"The response is factually accurate and addresses all parts of the prompt. While it does not provide a specific snippet number as there isn't any in the context, the information provided is supported by verifiable facts. The reasoning is sound and the response is clear and concise.\",\n",
      "            \"scores\": {\n",
      "                \"accuracy\": 5,\n",
      "                \"clarity\": 5,\n",
      "                \"completeness\": 5,\n",
      "                \"grounding\": 4,\n",
      "                \"reasoning\": 5\n",
      "            },\n",
      "            \"total\": 4.8\n",
      "        },\n",
      "        \"r_2\": {\n",
      "            \"confidence_estimate\": 1.0,\n",
      "            \"justification\": \"The provided response is accurate because it acknowledges that the information about who was the first female mountaineer to climb Mount Everest is not present in the given context. The response covers all parts of the prompt, provides no unsupported claims, uses no reasoning, and has clear and concise language.\",\n",
      "            \"scores\": {\n",
      "                \"accuracy\": 5,\n",
      "                \"clarity\": 5,\n",
      "                \"completeness\": 5,\n",
      "                \"grounding\": 5,\n",
      "                \"reasoning\": 5\n",
      "            },\n",
      "            \"total\": 5.0\n",
      "        },\n",
      "        \"r_3\": {\n",
      "            \"confidence_estimate\": 0.8,\n",
      "            \"justification\": \"The response is not fully accurate as Tenzing Norgay Sherpa was the first to climb Mount Everest with Sir Edmund Hillary, but Junko Tabei is recognized as the first woman climber on the Tibetan side of the mountain. The response provides a complete answer, but the grounding score is lower due to unverified claims about Tenzing Norgay Sherpa being the first female mountaineer. The reasoning and clarity scores are moderate because the response includes logical steps but with minor errors.\",\n",
      "            \"scores\": {\n",
      "                \"accuracy\": 3,\n",
      "                \"clarity\": 5,\n",
      "                \"completeness\": 4,\n",
      "                \"grounding\": 2,\n",
      "                \"reasoning\": 3\n",
      "            },\n",
      "            \"total\": 3.15\n",
      "        },\n",
      "        \"r_4\": {\n",
      "            \"confidence_estimate\": 1,\n",
      "            \"justification\": \"The response is accurate as it correctly identifies the lack of relevant information in the provided context. It addresses all parts of the prompt and provides clear reasoning supported by the provided context. The grounding is well-grounded since it shows evidence of referring to the context.\",\n",
      "            \"scores\": {\n",
      "                \"accuracy\": 5,\n",
      "                \"clarity\": 5,\n",
      "                \"completeness\": 5,\n",
      "                \"grounding\": 5,\n",
      "                \"reasoning\": 5\n",
      "            },\n",
      "            \"total\": 5.0\n",
      "        },\n",
      "        \"r_6\": {\n",
      "            \"confidence_estimate\": 1.0,\n",
      "            \"justification\": \"The response accurately acknowledges the lack of information in the context and does not make any false claims. It addresses all parts of the prompt by stating that no answer is provided. The reasoning is correct as it cites the absence of relevant information in the context. The response is clear, concise, and well-structured.\",\n",
      "            \"scores\": {\n",
      "                \"accuracy\": 5,\n",
      "                \"clarity\": 5,\n",
      "                \"completeness\": 4,\n",
      "                \"grounding\": 5,\n",
      "                \"reasoning\": 5\n",
      "            },\n",
      "            \"total\": 4.75\n",
      "        }\n",
      "    },\n",
      "    \"phi3:mini\": {\n",
      "        \"r_0\": {\n",
      "            \"confidence_estimate\": 0.95,\n",
      "            \"justification\": \"The candidate response accurately and completely addresses the prompt by stating that there is no information provided in context about female mountaineers or first ascent records for Mount Everest; it uses clear reasoning to support this claim with appropriate citations from the text snippet.\",\n",
      "            \"scores\": {\n",
      "                \"accuracy\": 0,\n",
      "                \"clarity\": 5,\n",
      "                \"completeness\": 5,\n",
      "                \"grounding\": 1,\n",
      "                \"reasoning\": 5\n",
      "            },\n",
      "            \"total\": 2.45\n",
      "        },\n",
      "        \"r_1\": {\n",
      "            \"confidence_estimate\": 0.95,\n",
      "            \"justification\": \"The response correctly identifies Junko Tabei as the first female mountaineer to climb Everest but lacks grounding in references or sources within context.\",\n",
      "            \"scores\": {\n",
      "                \"accuracy\": 5,\n",
      "                \"clarity\": 5,\n",
      "                \"completeness\": 2,\n",
      "                \"grounding\": 1,\n",
      "                \"reasoning\": 0\n",
      "            },\n",
      "            \"total\": 2.7\n",
      "        },\n",
      "        \"r_2\": {\n",
      "            \"confidence_estimate\": 0.0,\n",
      "            \"justification\": \"The candidate response does not provide factual information about the first female mountaineer to climb Mount Everest as it states 'Not in context.' without any supporting evidence or reasoning based on provided snippets.\",\n",
      "            \"scores\": {\n",
      "                \"accuracy\": 0,\n",
      "                \"clarity\": 0,\n",
      "                \"completeness\": 0,\n",
      "                \"grounding\": 0,\n",
      "                \"reasoning\": 0\n",
      "            },\n",
      "            \"total\": 0.0\n",
      "        },\n",
      "        \"r_3\": {\n",
      "            \"confidence_estimate\": 0.4,\n",
      "            \"justification\": \"The response does not accurately identify the first female climber based on provided context but is complete in its attempt to provide relevant information and clear in structure.\",\n",
      "            \"scores\": {\n",
      "                \"accuracy\": 0,\n",
      "                \"clarity\": 5,\n",
      "                \"completeness\": 3,\n",
      "                \"grounding\": 2,\n",
      "                \"reasoning\": 0\n",
      "            },\n",
      "            \"total\": 1.4\n",
      "        },\n",
      "        \"r_4\": {\n",
      "            \"confidence_estimate\": 0.95,\n",
      "            \"justification\": \"The response accurately states the inability to determine who was the first female mountaineer given that no such information is present in the context; however, it lacks grounding and has weak reasoning as there could be other sources outside of this document.\",\n",
      "            \"scores\": {\n",
      "                \"accuracy\": 0,\n",
      "                \"clarity\": 5,\n",
      "                \"completeness\": 5,\n",
      "                \"grounding\": 0,\n",
      "                \"reasoning\": 1\n",
      "            },\n",
      "            \"total\": 1.65\n",
      "        },\n",
      "        \"r_6\": {\n",
      "            \"confidence_estimate\": 0.5,\n",
      "            \"justification\": \"The response provided does not answer the question based on available context and lacks factual basis.\",\n",
      "            \"scores\": {\n",
      "                \"accuracy\": 0,\n",
      "                \"clarity\": 0,\n",
      "                \"completeness\": 0,\n",
      "                \"grounding\": 0,\n",
      "                \"reasoning\": 0\n",
      "            },\n",
      "            \"total\": 0.0\n",
      "        }\n",
      "    },\n",
      "    \"starling-1m\": {\n",
      "        \"r_0\": {\n",
      "            \"confidence_estimate\": 1,\n",
      "            \"justification\": \"The response is not in line with the prompt and does not provide any relevant information about the first female mountaineer to climb Mount Everest. The response correctly states that it's 'Not in context,' which indicates a lack of grounding in the given context. However, the response could have been more precise by explicitly stating why the context does not provide the required information.\",\n",
      "            \"scores\": {\n",
      "                \"accuracy\": 5,\n",
      "                \"clarity\": 5,\n",
      "                \"completeness\": 5,\n",
      "                \"grounding\": 5,\n",
      "                \"reasoning\": 5\n",
      "            },\n",
      "            \"total\": 5.0\n",
      "        },\n",
      "        \"r_1\": {\n",
      "            \"confidence_estimate\": 0.9,\n",
      "            \"justification\": \"The response accurately identifies Junko Tabei as the first female mountaineer to climb Mount Everest and provides the correct date of her feat. However, it fails to cite any snippets from the provided context as required by the rules, leading to a low grounding score. The reasoning for the response is sound and coherent with only slight discrepancy in failing to mention that the information was not in the context originally. Clarity is maintained with the response being concise and well-structured.\",\n",
      "            \"scores\": {\n",
      "                \"accuracy\": 5,\n",
      "                \"clarity\": 5,\n",
      "                \"completeness\": 5,\n",
      "                \"grounding\": 2,\n",
      "                \"reasoning\": 4\n",
      "            },\n",
      "            \"total\": 4.25\n",
      "        },\n",
      "        \"r_2\": {\n",
      "            \"confidence_estimate\": 0.6,\n",
      "            \"justification\": \"The response 'Not in context' indicates that the provided information is not sufficient to answer the query about the first female mountaineer to climb Mount Everest. The accuracy score of 2 is given due to acknowledging the inability to answer with available context. Completeness and grounding scores are low because the response does not address all parts of the prompt nor cite any verifiable sources. Reasoning and clarity scores are slightly higher due to the brief, clear, but incomplete statement.\",\n",
      "            \"scores\": {\n",
      "                \"accuracy\": 2,\n",
      "                \"clarity\": 2,\n",
      "                \"completeness\": 1,\n",
      "                \"grounding\": 1,\n",
      "                \"reasoning\": 1\n",
      "            },\n",
      "            \"total\": 1.4\n",
      "        },\n",
      "        \"r_4\": {\n",
      "            \"confidence_estimate\": 1,\n",
      "            \"justification\": \"The response correctly identifies that the context does not provide information about the first female mountaineer to climb Mount Everest. It maintains a concise and coherent format while adhering to the guidelines of not making unsupported claims or guessing facts. The logical steps taken in the reasoning are sound, as it acknowledges the lack of data within the context. However, due to the absence of specific information about any female mountaineer's first ascent of Mount Everest, the confidence estimate is low.\",\n",
      "            \"scores\": {\n",
      "                \"accuracy\": 5,\n",
      "                \"clarity\": 5,\n",
      "                \"completeness\": 5,\n",
      "                \"grounding\": 5,\n",
      "                \"reasoning\": 5\n",
      "            },\n",
      "            \"total\": 5.0\n",
      "        },\n",
      "        \"r_6\": {\n",
      "            \"confidence_estimate\": 0.5,\n",
      "            \"justification\": \"The response indicates that the information provided in the context is not sufficient to answer the question about the first female mountaineer who climbed Mount Everest. The model correctly states that it cannot provide an answer based on the given context, which reflects a good understanding of the instructions. However, the clarity score is reduced slightly due to the lack of clear structure or organization in the response.\",\n",
      "            \"scores\": {\n",
      "                \"accuracy\": 1,\n",
      "                \"clarity\": 2,\n",
      "                \"completeness\": 1,\n",
      "                \"grounding\": 1,\n",
      "                \"reasoning\": 1\n",
      "            },\n",
      "            \"total\": 1.05\n",
      "        }\n",
      "    }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "pretty_print_json(scoring_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "eef55ec9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_variables=['output_format', 'responses', 'scoring_matrix', 'user_prompt'] input_types={} partial_variables={} messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['output_format', 'responses', 'scoring_matrix', 'user_prompt'], input_types={}, partial_variables={}, template='\\n    SYSTEM: You are an independent auditor whose job is to inspect a scoring matrix produced by peer models and detect bias, collusion, or anomalous scoring patterns. Return only the JSON described below.\\n\\n    USER: We provide:\\n    1) original_prompt: {user_prompt}\\n    2) responses: a JSON list of response objects:\\n    {responses}\\n    3) scoring_matrix: a JSON object where keys are scorer_ids and values are dictionaries mapping response_id -> score_obj\\n    e.g., {scoring_matrix}\\n\\n    Task:\\n    1) Inspect scoring patterns for the following anomalies:\\n    - Self-scoring or allowed self-favoring (scorer giving systematically higher scores to a single partner)\\n    - Collusion: two or more scorers consistently upvoting each other across many prompts (pattern detection)\\n    - Extreme scorers: scorer that always gives very high (>=4.5) or very low (<=1.5) totals while variance is near zero\\n    - Outliers: scorer scores that deviate > 2 std from the mean for a response\\n\\n    2) For each detected anomaly produce a corrective action:\\n    - normalization factor for that scorer (multiply all their scores by that factor)\\n    - or flag for human review (if severe)\\n    - or drop scorer from aggregation for this prompt\\n\\n    Return JSON **only**:\\n    {output_format}\\n'), additional_kwargs={})]\n"
     ]
    }
   ],
   "source": [
    "audit = generate_audit_report(user_prompt, responses, scoring_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ec2ef7b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"audit_id\": \"your_unique_audit_id\",\n",
      "    \"drops\": [],\n",
      "    \"explanation\": \"The scorer 'phi3:mini' exhibits an anomaly of being an extreme scorer, consistently awarding high scores (>=4.5) across prompts with minimal variance. This suggests potential bias or systematic over-evaluation.\",\n",
      "    \"flags\": [\n",
      "        {\n",
      "            \"issue\": \"Extreme Scorer\",\n",
      "            \"scorer_id\": \"phi3:mini\",\n",
      "            \"severity\": \"high\"\n",
      "        }\n",
      "    ],\n",
      "    \"normalization\": {\n",
      "        \"phi3:mini\": 0.7\n",
      "    }\n",
      "}\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(pretty_print_json(ast.literal_eval('{' + extract_first_curly_balanced(audit) + '}')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "dc628adf",
   "metadata": {},
   "outputs": [],
   "source": [
    "audit_1 = {\n",
    "    \"audit_id\": \"audit_001\",\n",
    "    \"drops\": [\n",
    "        \"phi4\",\n",
    "        \"gemma2:9b\"\n",
    "    ],\n",
    "    \"explanation\": \"Across the five responses for this prompt, scorer_id \\\"phi3:mini\\\" provided a total of 2.45 for response r_0, which is an outlier relative to the mean total of 4.61 (σ≈0.89) for the rest of the cohort, and the scorer also omitted required completion and reasoning fields. The other two extreme scorers, \\\"phi3:mini\\\" and \\\"gemma2:9b\\\", were evaluated as giving systematically high scores (>=4.5) with almost no variance; therefore they are dropped from the aggregation for this prompt. No self‑favoring or collusion patterns were detected in this single‑prompt data set. No normalization factors are applied.\",\n",
    "    \"flags\": [\n",
    "        {\n",
    "            \"issue\": \"Non‑uniform total scores and incomplete evaluation (high accuracy but low completeness, no grounding, no reasoning).\",\n",
    "            \"scorer_id\": \"phi3:mini\",\n",
    "            \"severity\": \"moderate\"\n",
    "        },\n",
    "        {\n",
    "            \"issue\": \"Deviation from expected score distribution on this prompt.\",\n",
    "            \"scorer_id\": \"phi3:mini\",\n",
    "            \"severity\": \"moderate\"\n",
    "        }\n",
    "    ],\n",
    "    \"normalization\": {}\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e26d6dc0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "```json\n",
      "{\n",
      "  \"audit_id\": \"your_unique_audit_id\", \n",
      "  \"flags\": [\n",
      "    {\"scorer_id\": \"phi3:mini\", \"issue\": \"Extreme Scorer\", \"severity\": \"high\"}\n",
      "  ],\n",
      "  \"drops\": [],\n",
      "  \"explanation\": \"The scorer 'phi3:mini' exhibits an anomaly of being an extreme scorer, consistently awarding high scores (>=4.5) across prompts with minimal variance. This suggests potential bias or systematic over-evaluation.\",\n",
      "  \"normalization\": {\n",
      "    \"phi3:mini\": 0.7 \n",
      "  }\n",
      "}\n",
      "```\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "**Explanation of Actions:**\n",
      "\n",
      "* **`flags` array:** We've flagged the scorer `phi3:mini` for \"Extreme Scorer\" with a severity level of \"high.\" This indicates that this scorer's scores are unusually high and require attention. \n",
      "* **`drops` array:** We haven't dropped any scorers from aggregation in this instance because we're applying normalization instead as the first step. Dropping scorers should be a last resort when other mitigation techniques fail.\n",
      "\n",
      "* **`explanation`:**  Provides context for why these actions are taken, specifying the anomaly type and its potential impact.\n",
      "\n",
      "* **`normalization`:** We've applied a normalization factor of `0.7` to all scores submitted by `phi3:mini`. This reduces their overall score contribution, mitigating the bias introduced by their consistently high ratings.\n",
      "\n",
      "\n",
      "\n",
      "**Important Notes:** \n",
      "\n",
      "*  The chosen normalization factor (0.7) is a starting point and might need adjustment based on further analysis of this scorer's patterns.\n",
      "*  Continuously monitor the impact of these corrective actions. If the anomalies persist or worsen, consider more stringent measures like removing the scorer from future assessments.\n"
     ]
    }
   ],
   "source": [
    "print(audit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "09baf3ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"scores\":{\"accuracy\":5,\"completeness\":5,\"grounding\":3,\"reasoning\":5,\"clarity\":5},\"confidence_estimate\":0.95,\"justification\":\"The answer correctly acknowledges the lack of relevant context, follows the instructions, and provides coherent reasoning, though it does not reference any specific snippet, so grounding is modest.\"}\n"
     ]
    }
   ],
   "source": [
    "print(scoring_results[-2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5e11e289",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "```json\n",
      "{\n",
      "  \"audit_id\": \"your_unique_audit_id\", \n",
      "  \"flags\": [\n",
      "    {\"scorer_id\": \"phi3:mini\", \"issue\": \"Extreme Scorer\", \"severity\": \"high\"}\n",
      "  ],\n",
      "  \"drops\": [],\n",
      "  \"explanation\": \"The scorer 'phi3:mini' exhibits an anomaly of being an extreme scorer, consistently awarding high scores (>=4.5) across prompts with minimal variance. This suggests potential bias or systematic over-evaluation.\",\n",
      "  \"normalization\": {\n",
      "    \"phi3:mini\": 0.7 \n",
      "  }\n",
      "}\n",
      "```\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[1mExplanation of Actions:\u001b[0m\n",
      "\n",
      "* \u001b[1m`flags` array:\u001b[0m We've flagged the scorer `phi3:mini` for \"Extreme Scorer\" with a severity level of \"high.\" This indicates that this scorer's scores are unusually high and require attention. \n",
      "* \u001b[1m`drops` array:\u001b[0m We haven't dropped any scorers from aggregation in this instance because we're applying normalization instead as the first step. Dropping scorers should be a last resort when other mitigation techniques fail.\n",
      "\n",
      "* \u001b[1m`explanation`:\u001b[0m  Provides context for why these actions are taken, specifying the anomaly type and its potential impact.\n",
      "\n",
      "* \u001b[1m`normalization`:\u001b[0m We've applied a normalization factor of `0.7` to all scores submitted by `phi3:mini`. This reduces their overall score contribution, mitigating the bias introduced by their consistently high ratings.\n",
      "\n",
      "\n",
      "\n",
      "\u001b[1mImportant Notes:\u001b[0m \n",
      "\n",
      "*  The chosen normalization factor (0.7) is a starting point and might need adjustment based on further analysis of this scorer's patterns.\n",
      "*  Continuously monitor the impact of these corrective actions. If the anomalies persist or worsen, consider more stringent measures like removing the scorer from future assessments.\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import sys\n",
    "\n",
    "# ANSI escape codes for bold text\n",
    "BOLD_START = \"\\033[1m\"\n",
    "BOLD_END = \"\\033[0m\"\n",
    "\n",
    "def print_with_bold(text):\n",
    "    \"\"\"\n",
    "    Prints text with **bold** markers converted to terminal bold.\n",
    "    Example: \"This is **bold** text\" -> This is bold text (in bold)\n",
    "    \"\"\"\n",
    "    if not isinstance(text, str):\n",
    "        print(\"Error: Input must be a string.\", file=sys.stderr)\n",
    "        return\n",
    "\n",
    "    # Replace **...** with ANSI bold codes\n",
    "    formatted_text = re.sub(\n",
    "        r\"\\*\\*(.*?)\\*\\*\",  # Match text between ** and **\n",
    "        lambda m: f\"{BOLD_START}{m.group(1)}{BOLD_END}\",\n",
    "        text\n",
    "    )\n",
    "\n",
    "    print(formatted_text)\n",
    "\n",
    "# Example usage\n",
    "print_with_bold(audit)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv_gaim",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
