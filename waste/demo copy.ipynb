{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b93daa0c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(OllamaLLM(model='starling-lm'), 'starling-1m'),\n",
       " (OllamaLLM(model='ministral-3'), 'minstral-3'),\n",
       " (OllamaLLM(model='mistral:7b'), 'mistral:7b'),\n",
       " (OllamaLLM(model='phi3:mini'), 'phi3:mini'),\n",
       " (OllamaLLM(model='gemma2:9b'), 'gemma2:9b'),\n",
       " (OllamaLLM(model='gpt-oss:20b'), 'gpt-oss:20b')]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from ai_council.council import *\n",
    "from ai_council.prompts import *\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import PydanticOutputParser\n",
    "from ai_council.vector import *\n",
    "\n",
    "[(k[\"llm\"] , k[\"name\"]) for k in MODELS]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "00dc2e8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response from starling-1m\n",
      "Response generated by starling-1m with id r_0\n",
      "Response from minstral-3\n",
      "Response generated by minstral-3 with id r_1\n",
      "Response from mistral:7b\n",
      "Response generated by mistral:7b with id r_2\n",
      "Response from phi3:mini\n",
      "Response generated by phi3:mini with id r_3\n",
      "Response from gpt-oss:20b\n",
      "Response generated by gpt-oss:20b with id r_5\n",
      "{'response_id': 'r_0', 'model_id': 'expert_2', 'text': '1. Final answer: The document focuses on alignment of interest, governance, and transparency in private equity. It discusses reporting standards for quarterly and annual financial information, LPAC best practices, carry clawback considerations, and financial reporting.\\n2. Brief reasoning: The context provided outlines key principles and provides appendices discussing various aspects related to private equity practices. The document emphasizes the importance of clear communication with investors through regular reports on fund performance and portfolio company information.\\n3. Snippets used: #1 (Alignment of Interest, Governance, and Transparency), #2 (LPAC Best Practices), #3 (Carry Clawback Best Practice Considerations), #4 (Financial Reporting), #5 (Annual Reports), #6 (Quarterly Reports), #7 (LP Information), #8 (Portfolio Company Reports), and #9 (Capital Call and Distribution Notices).'}\n",
      "{'response_id': 'r_1', 'model_id': 'expert_3', 'text': '**Final answer:**\\nThe document outlines **ILPA Private Equity Principles**, focusing on governance, transparency, and financial reporting. It emphasizes quarterly and annual reporting standards for funds, portfolio companies, and LP information, with detailed appendices on best practices (e.g., LPAC, carry clawback).\\n\\n**Reasoning:**\\nThe context highlights three core themes: alignment of interest/governance/transparency (page 5), structured financial reporting (pages 14, 20, 21), and appendices for specialized guidance. Key reporting requirements include quarterly/unaudited P&L, portfolio company updates, and standardized LP information.\\n\\n**Snippets used:**\\n#1, #2, #3, #4'}\n",
      "{'response_id': 'r_2', 'model_id': 'expert_4', 'text': ' 1. The document discusses the ILPA Private Equity Principles, focusing on alignment of interest, governance, and transparency, with three appendices on LPAC Best Practices (Appendix A), Carry Clawback Best Practice Considerations (Appendix B), and Financial Reporting (Appendix C).\\n\\n   2. The document outlines the reporting requirements for funds, including annual and quarterly reports, and specific details about portfolio company reports and capital call and distribution notices.\\n\\n   3. Snippets used: 1, 4, 14, 20, 21'}\n",
      "{'response_id': 'r_3', 'model_id': 'expert_5', 'text': '1- The ILPA Private Equity Principles document, found in pages \\'5\\', \\'20\\', and \\'14\\' of a source titled \"AI Council Docs/ILPA_PE_principles.pdf\", provides guidelines for private equity reporting practices including alignment of interest, governance transparency, financial information disclosure, LP communication strategies, portfolio company reports specifications, capital call and distribution notices procedures along with appendices on best practices in these areas (appendix references \\'A\\', \\'B\\' & \\'C\\').\\n2- The document establishes reporting protocols aligned with ILPA Standardized Reporting principles for annual and quarterly fund activities. It emphasizes LP communication strategies, mandates detailed portfolio company reports including financial aspects like revenue, EBITDA etc., while also guiding on capital call procedures (page \\'14\\').\\n3- # 5#20#14'}\n",
      "{'response_id': 'r_5', 'model_id': 'expert_6', 'text': '**Summary:**  \\nThe ILPA Private Equity Principles document outlines governance and reporting standards, including LPAC best practices, carry clawback considerations, and financial reporting requirements such as annual and quarterly reports, detailed portfolio company reporting, and transparency on LPs and fund operations.  \\n\\n**Reasoning:**  \\nCombined excerpts cover alignment, appendices, and reporting guidelines.  \\n\\n**Snippets used:** 1, 2, 3, 4'}\n"
     ]
    }
   ],
   "source": [
    "vs = get_vector_db()\n",
    "input_prompt = \"Summarize the document\"\n",
    "context = vs.similarity_search(input_prompt)\n",
    "responses , user_prompt = generate_expert_response(input_prompt, context)\n",
    "for response in responses:\n",
    "    print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e368433e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starling-1m\n",
      "Could not parse r_0 by starling-1m due to expected string or bytes-like object, got 'NoneType'\n",
      "Could not parse r_1 by starling-1m due to 'scores'\n",
      "Could not parse r_2 by starling-1m due to expected string or bytes-like object, got 'NoneType'\n",
      "Could not parse r_3 by starling-1m due to unterminated string literal (detected at line 2) (<unknown>, line 2)\n",
      "Scoring complete for r_5 by starling-1m\n",
      "minstral-3\n",
      "Scoring complete for r_0 by minstral-3\n",
      "Scoring complete for r_1 by minstral-3\n",
      "Scoring complete for r_2 by minstral-3\n",
      "Scoring complete for r_3 by minstral-3\n",
      "Scoring complete for r_5 by minstral-3\n",
      "mistral:7b\n",
      "Scoring complete for r_0 by mistral:7b\n",
      "Scoring complete for r_1 by mistral:7b\n",
      "Scoring complete for r_2 by mistral:7b\n",
      "Scoring complete for r_3 by mistral:7b\n",
      "Scoring complete for r_5 by mistral:7b\n",
      "phi3:mini\n",
      "Scoring complete for r_0 by phi3:mini\n",
      "Scoring complete for r_1 by phi3:mini\n",
      "Scoring complete for r_2 by phi3:mini\n",
      "Scoring complete for r_3 by phi3:mini\n",
      "Scoring complete for r_5 by phi3:mini\n",
      "gpt-oss:20b\n",
      "Scoring complete for r_0 by gpt-oss:20b\n",
      "Scoring complete for r_1 by gpt-oss:20b\n",
      "Scoring complete for r_2 by gpt-oss:20b\n",
      "Scoring complete for r_3 by gpt-oss:20b\n",
      "Scoring complete for r_5 by gpt-oss:20b\n"
     ]
    }
   ],
   "source": [
    "scoring_matrix = generate_scores(responses, user_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "20649725",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_variables=['output_format', 'responses', 'scoring_matrix', 'user_prompt'] input_types={} partial_variables={} messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['output_format', 'responses', 'scoring_matrix', 'user_prompt'], input_types={}, partial_variables={}, template='\\n    SYSTEM: You are an independent auditor whose job is to inspect a scoring matrix produced by peer models and detect bias, collusion, or anomalous scoring patterns. Return only the JSON described below.\\n\\n    USER: We provide:\\n    1) original_prompt: {user_prompt}\\n    2) responses: a JSON list of response objects:\\n    {responses}\\n    3) scoring_matrix: a JSON object where keys are scorer_ids and values are dictionaries mapping response_id -> score_obj\\n    e.g., {scoring_matrix}\\n\\n    Task:\\n    1) Inspect scoring patterns for the following anomalies:\\n    - Self-scoring or allowed self-favoring (scorer giving systematically higher scores to a single partner)\\n    - Collusion: two or more scorers consistently upvoting each other across many prompts (pattern detection)\\n    - Extreme scorers: scorer that always gives very high (>=4.5) or very low (<=1.5) totals while variance is near zero\\n    - Outliers: scorer scores that deviate > 2 std from the mean for a response\\n\\n    2) For each detected anomaly produce a corrective action:\\n    - normalization factor for that scorer (multiply all their scores by that factor)\\n    - or flag for human review (if severe)\\n    - or drop scorer from aggregation for this prompt\\n\\n    Return JSON **only**:\\n    {output_format}\\n'), additional_kwargs={})]\n"
     ]
    }
   ],
   "source": [
    "audit , audit_prompt= generate_audit_report(input_prompt, responses, scoring_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3798b9db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "```json\n",
      "{\n",
      "  \"audit_id\": \"example-audit-123\",\n",
      "  \"flags\": [\n",
      "    {\n",
      "      \"scorer_id\": \"minstral-3\",\n",
      "      \"issue\": \"Self-scoring\",\n",
      "      \"severity\": \"High\"\n",
      "    }\n",
      "  ],\n",
      "  \"drops\": [],\n",
      "  \"explanation\": \"The scorer 'minstral-3' consistently gives significantly higher scores to response 'r_0' compared to other scorers. This suggests potential self-favoring or bias.\",\n",
      "  \"normalization\": {\n",
      "    \"minstral-3\": 0.85\n",
      "  }\n",
      "}\n",
      "``` \n",
      "\n",
      "\n",
      "\u001b[1mExplanation:\u001b[0m\n",
      "\n",
      "* \u001b[1m`audit_id`\u001b[0m: A unique identifier for this audit run.\n",
      "* \u001b[1m`flags`\u001b[0m: An array of objects, each detailing a detected anomaly. Here, we have one flag:\n",
      "    * `\"scorer_id\"`: The ID of the scorer exhibiting the issue (\"minstral-3\").\n",
      "    * `\"issue\"`:  A description of the anomaly (\"Self-scoring\").\n",
      "    * `\"severity\"`: A level indicating the seriousness of the issue (e.g., \"High\", \"Medium\", \"Low\").\n",
      "* \u001b[1m`drops`\u001b[0m: An array to list scorer IDs that are completely excluded from the aggregation due to severe issues.  This example doesn't use it as we only need normalization for one scorer.\n",
      "* \u001b[1m`explanation`\u001b[0m: A textual description of the audit findings and the rationale behind the actions taken. \n",
      "\n",
      "* \u001b[1m`normalization`\u001b[0m: A dictionary where keys are scorer IDs and values are the normalization factors. In this case, `minstral-3'`s scores will be multiplied by 0.85 to adjust for their potential bias.\n",
      "\n",
      "\n",
      "\n",
      "Let me know if you'd like me to elaborate on any specific anomaly or action!\n"
     ]
    }
   ],
   "source": [
    "print_with_bold(audit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a2d524fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "scoring_matrix, averages_json, best_response  = audited_scoring_matrix(audit, scoring_matrix, responses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "de62a2d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# response_ids = [k['response_id'] for k in responses]\n",
    "# audit_json = ast.literal_eval('{'+extract_first_curly_balanced(audit)+\"}\")\n",
    "# for model in audit_json['normalization']:\n",
    "#     for j in scoring_matrix[model]:\n",
    "#         scoring_matrix[model][j]['total'] *= audit_json['normalization'][model]\n",
    "# pretty_print_json(scoring_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5d9a562b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# scoring_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "281f21ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# responses[response_ids.index('r_1')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "01e8095f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# audit_json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "57223317",
   "metadata": {},
   "outputs": [],
   "source": [
    "# scoring_matrix[model][j]['total']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "604ab2aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import json\n",
    "\n",
    "# # your_data = {...}  # put your full dictionary here\n",
    "\n",
    "# def compute_average_totals(data):\n",
    "#     totals = {}      # collects all totals per response_id\n",
    "#     counts = {}      # counts how many values per response_id\n",
    "\n",
    "#     for model_data in data.values():\n",
    "#         for response_id, response_content in model_data.items():\n",
    "#             total_value = response_content[\"total\"]\n",
    "\n",
    "#             if response_id not in totals:\n",
    "#                 totals[response_id] = 0\n",
    "#                 counts[response_id] = 0\n",
    "\n",
    "#             totals[response_id] += total_value\n",
    "#             counts[response_id] += 1\n",
    "\n",
    "#     # compute averages\n",
    "#     averages = {response_id: totals[response_id] / counts[response_id]\n",
    "#                 for response_id in totals}\n",
    "\n",
    "#     return averages\n",
    "\n",
    "\n",
    "# # Example usage:\n",
    "# averages_json = compute_average_totals(scoring_matrix)\n",
    "# print(json.dumps(averages_json, indent=4))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "caf3f87c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# max(averages_json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "57453a49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print_with_bold(responses[response_ids.index(max(averages_json))]['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88950164",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv_gaim",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
