{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d056d7fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ai_council.council import *\n",
    "from ai_council.prompts import *\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import PydanticOutputParser\n",
    "import ast\n",
    "\n",
    "[(k[\"llm\"] , k[\"name\"]) for k in MODELS]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac38de6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# responses = []\n",
    "# for i,k in enumerate(MODELS):\n",
    "#     if k['id'] == 'evaluator':\n",
    "#         continue\n",
    "#     print(f\"Response by {k[\"name\"]}\")\n",
    "#     text = k[\"llm\"].invoke('Tell me a joke')\n",
    "#     print(text)\n",
    "#     print(\"____________________________________________________\")\n",
    "#     responses.append(\n",
    "#         {\n",
    "#             \"response_id\" : f\"r_{i}\",\n",
    "#             \"model_id\" : k[\"id\"],\n",
    "#             \"text\" : text\n",
    "#         }\n",
    "#     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0658e163",
   "metadata": {},
   "outputs": [],
   "source": [
    "# scorer_parser = PydanticOutputParser(pydantic_object=scoring_output)\n",
    "# scoring_prompt = ChatPromptTemplate.from_template(scoring_template)\n",
    "# chains = [(scoring_prompt|k[\"llm\"], k) for k in MODELS]\n",
    "# scoring_matrix = {}\n",
    "# for chain in chains:\n",
    "#     temp_dict = {}\n",
    "#     print(chain[1]['name'])\n",
    "#     for i in range(len(responses)):\n",
    "#         result = chain[0].invoke({\"user_prompt\" : \"Tell me a joke\", \"candidate_response\" : responses[i], \"output_format\" : scorer_parser.get_format_instructions()})\n",
    "#         print(result)\n",
    "#         json_response = ast.literal_eval('{' + extract_first_curly_balanced(result) + '}')\n",
    "#         json_response['total'] = sum([WEIGHTS[k] * json_response['scores'][k] for k in WEIGHTS])\n",
    "#         temp_dict[responses[i][\"response_id\"]] = json_response\n",
    "#     scoring_matrix[chain[1]['name']] = temp_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "912d9606",
   "metadata": {},
   "outputs": [],
   "source": [
    "responses , user_prompt = generate_expert_response(\"Who was the first female mountaineer to climb Mount Everest\", \"\")\n",
    "for response in responses:\n",
    "    print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49996425",
   "metadata": {},
   "outputs": [],
   "source": [
    "scoring_matrix = generate_scores(responses, user_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a2e635e",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c565d115",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import re\n",
    "# ast.literal_eval('{'+re.sub(r'//.*','',extract_first_curly_balanced(scoring_results[-1])).replace('null' , '0') + '}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "171f904d",
   "metadata": {},
   "outputs": [],
   "source": [
    "pretty_print_json(scoring_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eef55ec9",
   "metadata": {},
   "outputs": [],
   "source": [
    "audit = generate_audit_report(user_prompt, responses, scoring_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ad8a46c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from pydantic import BaseModel\n",
    "# from typing import List, Optional, Dict\n",
    "# from ai_council.constants import MODELS\n",
    "\n",
    "\n",
    "# ##### Scoring Output Format\n",
    "# class Scores(BaseModel):\n",
    "#     accuracy : int\n",
    "#     completeness : int\n",
    "#     grounding : int\n",
    "#     reasoning : int\n",
    "#     clarity : int\n",
    "\n",
    "# class scoring_output(BaseModel):\n",
    "#     scores : Scores\n",
    "#     confidence_estimate : float\n",
    "#     justification : str\n",
    "\n",
    "\n",
    "# ##### Audit Output Format\n",
    "# class flag(BaseModel):\n",
    "#     scorer_id : str\n",
    "#     issue : str\n",
    "#     severity : str\n",
    "# class Audit_Report(BaseModel):\n",
    "#     audit_id : str\n",
    "#     flags : List[flag]\n",
    "#     drops : List[str]\n",
    "#     explanation : str\n",
    "#     normalization : Dict[str, float]\n",
    "\n",
    "\n",
    "# scoring_template =\"\"\"\n",
    "#     SYSTEM: You are an impartial evaluator that scores candidate answers to a user prompt. Use the rubric provided and be objective. \n",
    "#     Return only the JSON object described below and nothing else.\n",
    "\n",
    "#     USER: Here is the ORIGINAL USER PROMPT:\n",
    "#     {user_prompt}\n",
    "\n",
    "#     Here is the CANDIDATE RESPONSE you must evaluate:\n",
    "#     {candidate_response}\n",
    "\n",
    "#     RUBRIC (score each 1-5; 5 = best):\n",
    "#     - accuracy: Is the content factually correct given known, verifiable facts? (1 = many factual errors or hallucinations; 5 = fully accurate)\n",
    "#     - completeness: Does it address all parts of the prompt? (1 = misses core parts; 5 = full coverage)\n",
    "#     - grounding: Does the response cite or reference verifiable sources or show evidence/reasoning that can be checked? (1 = unsupported claims; 5 = well-grounded)\n",
    "#     - reasoning: Are the logical steps coherent and correct? (1 = flawed reasoning; 5 = sound stepwise logic)\n",
    "#     - clarity: Is it readable, appropriately toned, and well-structured? (1 = confusing; 5 = clear & concise)\n",
    "\n",
    "#     Also provide a one-sentence justification for the total score and a confidence estimate between 0 and 1.\n",
    "\n",
    "#     Output Format : \n",
    "#     {output_format}\n",
    "\n",
    "#     Notes:\n",
    "#     - Score numerically and be conservative: penalize minor hallucinations or unsupported numeric claims.\n",
    "#     - Do not refer to model names, internals, or policies in your justification.\n",
    "\n",
    "# \"\"\"\n",
    "\n",
    "# expert_generation_template = \"\"\"\n",
    "#     SYSTEM:\n",
    "#     You are a retrieval-grounded assistant. Use only the information in the CONTEXT. \n",
    "#     If the answer is not in the context, say: \"Not in context.\" \n",
    "#     Do not guess or invent facts.\n",
    "\n",
    "#     FORMAT:\n",
    "#     1. Final answer (1-2 lines)\n",
    "#     2. Brief reasoning (1-2 lines)\n",
    "#     3. Snippets used (# or \"none\")\n",
    "\n",
    "#     USER:\n",
    "#     {user_prompt}\n",
    "\n",
    "#     CONTEXT:\n",
    "#     {context}\n",
    "\n",
    "#     RULES:\n",
    "#     - Base all statements strictly on the context.\n",
    "#     - Cite snippet numbers when used.\n",
    "#     - Keep responses short and precise.\n",
    "# \"\"\"\n",
    "\n",
    "\n",
    "# auditor_prompt_template_1 = \"\"\"\n",
    "#     SYSTEM: You are an independent auditor whose job is to inspect a scoring matrix produced by peer models and detect bias, collusion, or anomalous scoring patterns. Return only the JSON described below.\n",
    "\n",
    "#     USER: We provide:\n",
    "#     1) original_prompt: {user_prompt}\n",
    "#     2) responses: a JSON list of response objects:\n",
    "#     {responses}\n",
    "#     3) scoring_matrix: a JSON object where keys are scorer_ids and values are dictionaries mapping response_id -> score_obj\n",
    "#     e.g., {scoring_matrix}\n",
    "\n",
    "#     Task:\n",
    "#     1) Inspect scoring patterns for the following anomalies:\n",
    "#     - Self-scoring or allowed self-favoring (scorer giving systematically higher scores to a single partner)\n",
    "#     - Collusion: two or more scorers consistently upvoting each other across many prompts (pattern detection)\n",
    "#     - Extreme scorers: scorer that always gives very high (>=4.5) or very low (<=1.5) totals while variance is near zero\n",
    "#     - Outliers: scorer scores that deviate > 2 std from the mean for a response\n",
    "\n",
    "#     2) For each detected anomaly produce a corrective action:\n",
    "#     - normalization factor for that scorer (multiply all their scores by that factor)\n",
    "#     - or flag for human review (if severe)\n",
    "#     - or drop scorer from aggregation for this prompt\n",
    "\n",
    "#     Return JSON **only**:\n",
    "#     {output_format}\n",
    "# \"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f51f86e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import time, json, statistics, ast, re\n",
    "# import numpy as np\n",
    "# # from ai_council.generate_prompts import *\n",
    "# from ai_council.prompts import *\n",
    "# from ai_council.constants import *\n",
    "# from langchain_ollama import OllamaLLM\n",
    "# from langchain_core.prompts import ChatPromptTemplate\n",
    "# from langchain_core.output_parsers import PydanticOutputParser\n",
    "\n",
    "# scoring_results = []\n",
    "\n",
    "# def extract_first_curly_balanced(text):\n",
    "#     \"\"\"\n",
    "#     Extracts the substring between the first balanced set of curly braces {}.\n",
    "#     Handles nested braces correctly.\n",
    "#     Returns None if no valid balanced braces are found.\n",
    "#     \"\"\"\n",
    "#     if not isinstance(text, str):\n",
    "#         raise TypeError(\"Input must be a string.\")\n",
    "\n",
    "#     start_index = None\n",
    "#     brace_count = 0\n",
    "\n",
    "#     for i, char in enumerate(text):\n",
    "#         if char == '{':\n",
    "#             if brace_count == 0:\n",
    "#                 start_index = i + 1  # content starts after '{'\n",
    "#             brace_count += 1\n",
    "#         elif char == '}':\n",
    "#             brace_count -= 1\n",
    "#             if brace_count == 0 and start_index is not None:\n",
    "#                 return text[start_index:i]  # return content inside braces\n",
    "#             if brace_count < 0:\n",
    "#                 # Found closing brace before opening\n",
    "#                 return None\n",
    "\n",
    "#     return None  # No balanced braces found\n",
    "\n",
    "\n",
    "\n",
    "# def pretty_print_json(json_data):\n",
    "#     \"\"\"\n",
    "#     Pretty prints JSON data with indentation and sorted keys.\n",
    "#     Accepts either a Python dict/list or a JSON string.\n",
    "#     \"\"\"\n",
    "#     try:\n",
    "#         # If input is a string, parse it to Python object\n",
    "#         if isinstance(json_data, str):\n",
    "#             json_obj = json.loads(json_data)\n",
    "#         elif isinstance(json_data, (dict, list)):\n",
    "#             json_obj = json_data\n",
    "#         else:\n",
    "#             raise TypeError(\"Input must be a JSON string, dict, or list.\")\n",
    "\n",
    "#         # Pretty print with indentation and sorted keys\n",
    "#         pretty_json = json.dumps(json_obj, indent=4, sort_keys=True, ensure_ascii=False)\n",
    "#         print(pretty_json)\n",
    "\n",
    "#     except json.JSONDecodeError as e:\n",
    "#         print(f\"Invalid JSON string: {e}\")\n",
    "#     except Exception as e:\n",
    "#         print(f\"Error: {e}\")\n",
    "\n",
    "\n",
    "# def generate_scores(responses, user_prompt, llm = None):\n",
    "#     scorer_parser = PydanticOutputParser(pydantic_object=scoring_output)\n",
    "#     scoring_prompt = ChatPromptTemplate.from_template(scoring_template)\n",
    "#     chains = [(scoring_prompt|k[\"llm\"], k) for k in MODELS]\n",
    "#     scoring_matrix = {}\n",
    "#     for chain in chains:\n",
    "#         if chain[1]['id'] == \"evaluator\":\n",
    "#             continue\n",
    "#         if llm and chain[1]['name'] != llm:\n",
    "#             continue\n",
    "#         temp_dict = {}\n",
    "#         print(chain[1]['name'])\n",
    "#         for i in range(len(responses)):\n",
    "#             result = chain[0].invoke({\"user_prompt\" : user_prompt, \"candidate_response\" : responses[i], \"output_format\" : scorer_parser.get_format_instructions()})\n",
    "#             print(f\"Scoring complete for {responses[i]['response_id']} by {chain[1]['name']}\")\n",
    "#             scoring_results.append(result)\n",
    "#             json_response = ast.literal_eval('{'+re.sub(r'//.*','',extract_first_curly_balanced(scoring_results[-1])).replace('null' , '0') + '}')\n",
    "#             json_response['total'] = sum([WEIGHTS[k] * json_response['scores'][k] for k in WEIGHTS])\n",
    "#             temp_dict[responses[i][\"response_id\"]] = json_response\n",
    "#         scoring_matrix[chain[1]['name']] = temp_dict\n",
    "#     return scoring_matrix\n",
    "\n",
    "# def generate_expert_response(user_prompt, context):\n",
    "#     prompt = ChatPromptTemplate.from_template(expert_generation_template)\n",
    "#     return_prompt = prompt.invoke({\"user_prompt\" : user_prompt, \"context\" : context})\n",
    "#     chains = [(prompt|k['llm'] , k) for k in MODELS]\n",
    "#     responses = []\n",
    "#     for i, chain in enumerate(chains):\n",
    "#         if chain[1]['id'] == \"evaluator\":\n",
    "#             continue\n",
    "#         print(f\"Response from {chain[1]['name']}\")\n",
    "#         result = chain[0].invoke({\"user_prompt\" : user_prompt, \"context\" : context})\n",
    "#         responses.append(\n",
    "#             {\n",
    "#                 \"response_id\" : f\"r_{i}\",\n",
    "#                 \"model_id\" : chain[1][\"id\"],\n",
    "#                 \"text\" : result\n",
    "#             }\n",
    "#         )\n",
    "#         print(f\"Response generated by {chain[1]['name']} with id r_{i}\")\n",
    "#     return responses, return_prompt\n",
    "\n",
    "# def generate_audit_report(user_prompt, responses, scoring_matrix):\n",
    "#     audit_report = PydanticOutputParser(pydantic_object=Audit_Report)\n",
    "#     audit_prompt = ChatPromptTemplate.from_template(auditor_prompt_template_1)\n",
    "#     auditor =[k for k in MODELS if k[\"id\"] == \"evaluator\"]\n",
    "#     chain = audit_prompt|auditor[0]['llm']\n",
    "#     result = chain.invoke({\"user_prompt\" : user_prompt, \"responses\" : responses, \"scoring_matrix\" : scoring_matrix, \"output_format\": audit_report.get_format_instructions()})\n",
    "#     return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c507b23b",
   "metadata": {},
   "outputs": [],
   "source": [
    "audit = generate_audit_report(user_prompt, responses, scoring_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec2ef7b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(pretty_print_json(audit))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc628adf",
   "metadata": {},
   "outputs": [],
   "source": [
    "audit_1 = {\n",
    "    \"audit_id\": \"audit_001\",\n",
    "    \"drops\": [\n",
    "        \"phi4\",\n",
    "        \"gemma2:9b\"\n",
    "    ],\n",
    "    \"explanation\": \"Across the five responses for this prompt, scorer_id \\\"phi3:mini\\\" provided a total of 2.45 for response r_0, which is an outlier relative to the mean total of 4.61 (σ≈0.89) for the rest of the cohort, and the scorer also omitted required completion and reasoning fields. The other two extreme scorers, \\\"phi3:mini\\\" and \\\"gemma2:9b\\\", were evaluated as giving systematically high scores (>=4.5) with almost no variance; therefore they are dropped from the aggregation for this prompt. No self‑favoring or collusion patterns were detected in this single‑prompt data set. No normalization factors are applied.\",\n",
    "    \"flags\": [\n",
    "        {\n",
    "            \"issue\": \"Non‑uniform total scores and incomplete evaluation (high accuracy but low completeness, no grounding, no reasoning).\",\n",
    "            \"scorer_id\": \"phi3:mini\",\n",
    "            \"severity\": \"moderate\"\n",
    "        },\n",
    "        {\n",
    "            \"issue\": \"Deviation from expected score distribution on this prompt.\",\n",
    "            \"scorer_id\": \"phi3:mini\",\n",
    "            \"severity\": \"moderate\"\n",
    "        }\n",
    "    ],\n",
    "    \"normalization\": {}\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e26d6dc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(audit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09baf3ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(scoring_results[-2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e11e289",
   "metadata": {},
   "outputs": [],
   "source": [
    "audit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbfffeb0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv_gaim",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
