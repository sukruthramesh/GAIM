import time, json, statistics, ast, re
import numpy as np
# from ai_council.generate_prompts import *
from ai_council.prompts import *
from ai_council.constants import *
from langchain_ollama import OllamaLLM
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import PydanticOutputParser

scoring_results = []

def extract_first_curly_balanced(text):
    """
    Extracts the substring between the first balanced set of curly braces {}.
    Handles nested braces correctly.
    Returns None if no valid balanced braces are found.
    """
    if not isinstance(text, str):
        raise TypeError("Input must be a string.")

    start_index = None
    brace_count = 0

    for i, char in enumerate(text):
        if char == '{':
            if brace_count == 0:
                start_index = i + 1  # content starts after '{'
            brace_count += 1
        elif char == '}':
            brace_count -= 1
            if brace_count == 0 and start_index is not None:
                return text[start_index:i]  # return content inside braces
            if brace_count < 0:
                # Found closing brace before opening
                return None

    return None  # No balanced braces found



def pretty_print_json(json_data):
    """
    Pretty prints JSON data with indentation and sorted keys.
    Accepts either a Python dict/list or a JSON string.
    """
    try:
        # If input is a string, parse it to Python object
        if isinstance(json_data, str):
            json_obj = json.loads(json_data)
        elif isinstance(json_data, (dict, list)):
            json_obj = json_data
        else:
            raise TypeError("Input must be a JSON string, dict, or list.")

        # Pretty print with indentation and sorted keys
        pretty_json = json.dumps(json_obj, indent=4, sort_keys=True, ensure_ascii=False)
        print(pretty_json)

    except json.JSONDecodeError as e:
        print(f"Invalid JSON string: {e}")
    except Exception as e:
        print(f"Error: {e}")


def generate_scores(responses, user_prompt, llm = None):
    scorer_parser = PydanticOutputParser(pydantic_object=scoring_output)
    scoring_prompt = ChatPromptTemplate.from_template(scoring_template)
    chains = [(scoring_prompt|k["llm"], k) for k in MODELS]
    scoring_matrix = {}
    for chain in chains:
        if chain[1]['id'] == "evaluator":
            continue
        if llm and chain[1]['name'] != llm:
            continue
        temp_dict = {}
        print(chain[1]['name'])
        for i in range(len(responses)):
            result = chain[0].invoke({"user_prompt" : user_prompt, "candidate_response" : responses[i], "output_format" : scorer_parser.get_format_instructions()})
            print(f"Scoring complete for {responses[i]['response_id']} by {chain[1]['name']}")
            try:
                scoring_results.append(result)
                json_response = ast.literal_eval('{'+re.sub(r'//.*','',extract_first_curly_balanced(scoring_results[-1])).replace('null' , '0') + '}')
                json_response['total'] = sum([WEIGHTS[k] * json_response['scores'][k] for k in WEIGHTS])
                temp_dict[responses[i]["response_id"]] = json_response
            except Exception as e:
                print(f"Could not parse {responses[i]['response_id']} by {chain[1]['name']} due to {e}")
        scoring_matrix[chain[1]['name']] = temp_dict
    return scoring_matrix

def generate_expert_response(user_prompt, context):
    prompt = ChatPromptTemplate.from_template(expert_generation_template)
    return_prompt = prompt.invoke({"user_prompt" : user_prompt, "context" : context})
    chains = [(prompt|k['llm'] , k) for k in MODELS]
    responses = []
    for i, chain in enumerate(chains):
        if chain[1]['id'] == "evaluator":
            continue
        print(f"Response from {chain[1]['name']}")
        result = chain[0].invoke({"user_prompt" : user_prompt, "context" : context})
        responses.append(
            {
                "response_id" : f"r_{i}",
                "model_id" : chain[1]["id"],
                "text" : result
            }
        )
        print(f"Response generated by {chain[1]['name']} with id r_{i}")
    return responses, return_prompt

def generate_audit_report(user_prompt, responses, scoring_matrix):
    audit_report = PydanticOutputParser(pydantic_object=Audit_Report)
    audit_prompt = ChatPromptTemplate.from_template(auditor_prompt_template)
    auditor =[k for k in MODELS if k["id"] == "evaluator"]
    chain = audit_prompt|auditor[0]['llm']
    result = chain.invoke({"user_prompt" : user_prompt, "responses" : responses, "scoring_matrix" : scoring_matrix, "output_format": audit_report.get_format_instructions()})
    return result